{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vwXPowGD_tL"
   },
   "source": [
    "# **COMP9727 Recommender Systems**\n",
    "## Tutorial Week 5: Sentiment Analysis\n",
    "\n",
    "@Author: **Mingqin Yu**\n",
    "\n",
    "@Reviewer: **Wayne Wobcke**\n",
    "\n",
    "### Objective\n",
    "\n",
    "The aim of the tutorial is to study the use of a neural network architecture, in particular Word2Vec with a Multi-Layer Perceptron, for sentiment analysis, a widely used technology underpinning recommender systems.\n",
    "\n",
    "### Before the tutorial\n",
    "\n",
    "1. Review the lecture material on Word2Vec, Multi-Layer Perceptrons and backpropagation.\n",
    "\n",
    "2. Read, understand and run all the code in the Neural Network pipeline below, and come prepared to discuss the answers to some of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2WaYF6hFUsD"
   },
   "source": [
    "## Neural Network Pipeline\n",
    "\n",
    "1. **Environment Setup**\n",
    "   - Install libraries and tools\n",
    "   - Load and decode the IMDb dataset\n",
    "   - Tokenize the reviews\n",
    "2. **Word2Vec Word Embeddings**\n",
    "   - Train the Word2Vec model\n",
    "   - Visualize word embeddings with t-SNE\n",
    "3. **Multi-Layer Perceptron Architecture**\n",
    "   - Preprocessing for neural network\n",
    "   - Define neural network architecture\n",
    "   - Train the model and visualize performance\n",
    "\n",
    "**Specific Learning Objectives**\n",
    "\n",
    "1. **Understand**\n",
    "   - How to buid a neural network model for word embeddings\n",
    "   - A possible application of neural networks for sentiment analysis\n",
    "2. **Apply**\n",
    "   - Use a deep learning package for defining layered neural network architectures\n",
    "   - Use word embeddings as an input leyer in a neural network model\n",
    "   - Use a multi-layer perceptron for classification\n",
    "3. **Analyse**\n",
    "   - Interpret training and validation set accuracy\n",
    "4. **Evaluate**\n",
    "   - The effectiveness of the sentiment classifier\n",
    "   - Visualize and interpret the word embeddings\n",
    "   - Visualize the results at each stage of the tutorial for deeper comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcQYzSHYFhoh"
   },
   "source": [
    "## Neural Network Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7xDKRPeGIAq"
   },
   "source": [
    "### 1. Environment Setup\n",
    "\n",
    "__Step 1. Install libraries and packages__\n",
    "\n",
    "The following libraries and datasets are used\n",
    "- `Word2Vec` from `gensim` is used to generate word embeddings\n",
    "- `imdb` dataset from `keras`, which is a set of movie reviews labeled as positive or negative\n",
    "- `pad_sequences` is a utility function from `keras` used to ensure input sequences have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfnmKg004iWT"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kHKKGIGGTbI"
   },
   "source": [
    "__Step 2. Load and Decode the IMDb Dataset__\n",
    "\n",
    "In this section, you are working with the IMDb movie reviews dataset\n",
    "\n",
    "**Load the IMDb dataset**\n",
    "- **`imdb.load_data(num_words=10000)`**\n",
    "    - This function fetches the IMDb dataset from `keras`\n",
    "    - The `num_words=10000` parameter specifies that you only want to load the top 10,000 most frequent words from the dataset, which is used or reducing the dataset's size and ensuring the model focuses on the most relevant words\n",
    "- **`imdb.get_word_index()`**\n",
    "    - This function retrieves the dictionary mapping of words to their corresponding index in the dataset\n",
    "  \n",
    "**Decode the reviews**\n",
    "- Movies reviews in the dataset are represented as sequences of integers. To understand and work with these reviews, they need to be converted to a textual format\n",
    "- `word_index` maps integers to their respective words. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for \"padding\", \"start of sequence\" and \"unknown\"\n",
    "\n",
    "**Tokenize the reviews**\n",
    "- The reviews are broken down into individual words, which is essential for the Word2Vec model\n",
    "- `review.split()` breaks the review string into a list of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_oSH6woGWO_"
   },
   "outputs": [],
   "source": [
    "# Load keras IMDb dataset\n",
    "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Decode reviews\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_reviews = [' '.join([reverse_word_index.get(i - 3, '?') for i in review]) for review in X_train_raw]\n",
    "\n",
    "# Tokenize reviews\n",
    "tokenized_reviews = [review.split() for review in decoded_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0WAPvPOG06Z"
   },
   "source": [
    "### 2. Word2Vec Word Embeddings\n",
    "\n",
    "Word embeddings represent words in a dense vector format where semantically similar words are closer in the vector space\n",
    "\n",
    "__Step 1. Train the Word2Vec model__\n",
    "\n",
    "- **`Word2Vec()`**\n",
    "  - `sentences=tokenized_reviews`: Pass the tokenized movie reviews to the model\n",
    "  - `vector_size=100`: Each word will be represented as a 100-dimensional vector\n",
    "  - `window=5`: The model will look at a window of 5 words at a time (i.e. the current word and 2 words on each side) to learn the representations\n",
    "  - `min_count=1`: Words that appear only once in the entire dataset will also be considered\n",
    "  - `workers=4`: Uses 4 CPU cores to speed up model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model_w2v = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRuCYfSoGd_q"
   },
   "source": [
    "__Step 2. Visualize Word Embeddings__\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a popular algorithm to visualize high-dimensional data in 2D or 3D\n",
    "\n",
    "- **t-SNE**:\n",
    "  - First, select a sample of words\n",
    "  - Convert their embeddings into 2D coordinates using t-SNE\n",
    "  - Plot these coordinates to see how words relate to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "sIP6vl2K4jBM",
    "outputId": "76cdaec7-3175-4fe5-b2ca-1c2c9393dc30"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select sample words\n",
    "sample_words = ['film', 'actor', 'cast', 'great', 'good', 'bad', 'awful', 'terrible', 'boring']\n",
    "sample_vectors = [model_w2v.wv[word] for word in sample_words if word in model_w2v.wv.index_to_key]\n",
    "\n",
    "# Convert list of vectors to a 2D array\n",
    "sample_vectors = np.array(sample_vectors)\n",
    "\n",
    "# Set perplexity to less than the number of sample_vectors\n",
    "perplexity_value = len(sample_vectors) - 1\n",
    "\n",
    "# Use t-SNE with adjusted perplexity\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=0)\n",
    "coordinates = tsne.fit_transform(sample_vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, word in enumerate(sample_words[:len(sample_vectors)]):  # Adjust the loop to the number of vectors\n",
    "    plt.scatter(coordinates[i, 0], coordinates[i, 1])\n",
    "    plt.annotate(word, xy=(coordinates[i, 0], coordinates[i, 1]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-VKqnrSHHea"
   },
   "source": [
    "### 3. Multi-Layer Perceptron Architecture \n",
    "\n",
    "__Step 1. Preprocessing for Neural Network__\n",
    "\n",
    "Neural networks require inputs to be of consistent size. Thus, movie reviews need to be of the same length.\n",
    "- **`pad_sequences`**\n",
    "  - This function ensures that all sequences in a list have the same length, either by padding them with a value (default is 0) or truncating them\n",
    "  - `maxlen=500`: Reviews longer than 500 words will be truncated, and shorter ones will be padded\n",
    "- **Embedding matrix**\n",
    "  - An embedding matrix is a weight matrix that will be loaded into the `keras` embedding layer using the vectors learned by the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4osJbPc4lb6"
   },
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "maxlen = 500\n",
    "X_train = pad_sequences(X_train_raw, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_raw, maxlen=maxlen)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((10000, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in model_w2v.wv.index_to_key:\n",
    "        embedding_matrix[i] = model_w2v.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHfBNfnRHQgj"
   },
   "source": [
    "__Step 2. Define Neural Network Architecture__\n",
    "\n",
    "Construct a neural network in `keras` to predict the sentiment of movie reviews\n",
    "\n",
    "- **Model layers**\n",
    "  - `Embedding layer`: Convert word indices into dense vectors of fixed size, here, vectors of size 100\n",
    "  - `Flatten layer`: Flattens the input. For instance, if the input is (batch_size, maxlen, 100), the output would be (batch_size, maxlen*100)\n",
    "  - `Dense layers`: Fully connected neural network layers\n",
    "  - `Dropout`: Randomly sets a fraction (here 0.5 or 50%) of input units to 0 at each update during training to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvSrTU5i4o3V",
    "outputId": "89c7b05b-db88-4f15-997d-9c1bbfaa924f"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 100, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ohgXGW5HU8y"
   },
   "source": [
    "__Step 3. Train the Model and Visualize Performance__\n",
    "\n",
    "Visualize the model's performance over time\n",
    "- **Plot**\n",
    "  - Using the history of the model training, plot the accuracy for both training and validation data across epochs. This visualization helps determine if the model is overfitting (when training accuracy increases but validation accuracy starts decreasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "RIXqH3G44rly",
    "outputId": "bd1e7125-9433-45b8-b465-856055e13407"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Visualization of training and validation set accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49a-TQidHaYR"
   },
   "source": [
    "## Discussion Questions\n",
    "\n",
    "1. **Parameter Tuning**\n",
    "    - How do different values of the `window` parameter in Word2Vec impact the quality of embeddings?\n",
    "    - Briefly describe what the Adam optimizer does\n",
    "2. **Methods**\n",
    "    - Find out which version of Word2Vec is used by `gensim` and whether another variety of Word2Vec is better\n",
    "    - Experiment with pretrained Word2Vec models such as Google News to see if results improve\n",
    "    - Apart from Word2Vec, what other sentiment analysis techniques can you apply in this context? How would they compare?\n",
    "3. **Model Evaluation**\n",
    "    - What do the visualizations tell you about the effectiveness of the models, for both the word embeddings and the sentiment classifier?\n",
    "    - Experiment with some domain specific sentiment words to see how the t-SNE plot for the word embeddings changes\n",
    "    - Is the data of sufficient quantity and quality for the method to learn a good model?\n",
    "    - How could you further preprocess data to enhance the quality of embeddings?\n",
    "    - What other layers or activation functions could you use in the neural network to improve accuracy?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
