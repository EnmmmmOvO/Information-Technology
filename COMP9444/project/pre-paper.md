We also implemented a model based on multi-model feature extraction, utilizing ResNet, Virsion Transformer, and Swin transformer, extracting the final layer weights. We then used a modified IFCNN with attention selection algorithms to combine these features, extracting the weights from the three models. According to the attention part output weight, we will processing the original picture, The high-weight parts were retained, while the low-weight parts were directly converted to black. Which aims to focus on important part. Finally, use SVM to classificate the processed image. 

However, we find several challenges with this model. Training was difficult, we need train three models (ResNet, ViT, and Swin-T) before training the attention part. Additionally, the attention model couldn't be directly compared with class outputs and required to mark image position weights by ourselves. Inspired by another paper, we attempted to use a VAE to address this issue.

The VAE comprises an encoder and a decoder. Upon inputting an image, the encoder compresses it, and the compressed content is provided to the decoder for expansion. The loss between the input and output is then compared, eliminating the need for manual intervention. After training, We attempted to use the encoder data as input for the SVM, which offered a better solution.

Our dataset included not only insects but also plants affected by pests. Therefore, a VAE, unlike when generating faces with a clear target, the losing is high, and accuracy is a low than resent and transformer which good at feature extraction 