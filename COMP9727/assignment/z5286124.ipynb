{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# COMP9727 Recommender Systems 24T2 Assignment\n",
    "\n",
    "Name: Jinghan Wang  \n",
    "Student ID: z5286124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Part 1 Topic (Genre) Classification\n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dictionary = dict()\n",
    "file_list = [os.path.join('dataset', file) for file in os.listdir('dataset') if file.endswith('.tsv')]\n",
    "\n",
    "# Concatenate all the tsv files into one dataframe\n",
    "df = pd.concat([pd.read_csv(file, sep='\\t') for file in file_list], ignore_index=True)\n",
    "\n",
    "# Combine all columns into one column 'text'\n",
    "df['text'] = df['Title'].astype(str) + ' ' + df['Release Year'].astype(str) + ' ' + df['Genre'].astype(str) + ' ' + df['Director'].astype(str) + ' ' + df['Cast'].astype(str) + ' ' + df['Plot'].astype(str) + ' ' + df['Origin/Ethnicity'].astype(str)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "sklearn_stop_words = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "def preprocess_text(text, true=False, stop_words=nltk_stop_words):\n",
    "    text = text.lower()\n",
    "    if true:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    else:\n",
    "        text = re.sub(r'[^\\w\\s\\'.,!?@#&$%\\-+*/=]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Evaluation of Punctuation Retention in NLP Models\n",
    "\n",
    "In the tutorial code, the expression `[^\\\\w\\\\s]` is used to remove all characters that are not spaces or alphanumeric. However, in the context of **NLP**, punctuation can convey specific meanings, and symbols like `$`, `=` etc., have special significance. Removing them entirely can affect the effectiveness of NLP. Therefore, I tested two scenarios using the **MNB** and **BNB** models: one where all punctuation is removed, and one where punctuation is retained. Using _5-fold cross-validation_, the mean and standard deviation of **f1_macro**, **f1_micro**, and **accuracy** were calculated to evaluate the effectiveness in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Scenario 1: Remove all punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda text : preprocess_text(text, True))\n",
    "\n",
    "# Use cross-validation to evaluate the performance of the models, instead of a single train-test split\n",
    "cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bnb = Pipeline(steps=[('preprocessor', CountVectorizer()), ('classifier', BernoulliNB())])\n",
    "mnb = Pipeline(steps=[('preprocessor', CountVectorizer()), ('classifier', MultinomialNB())])\n",
    "\n",
    "def get_list(mnb_score, bnb_score):\n",
    "    return [\n",
    "        cross_val_score(bnb_score, df['text'], df['Genre'], cv=cv, scoring='f1_macro'),\n",
    "        cross_val_score(mnb_score, df['text'], df['Genre'], cv=cv, scoring='f1_macro'),\n",
    "        cross_val_score(bnb_score, df['text'], df['Genre'], cv=cv, scoring='accuracy'),\n",
    "        cross_val_score(mnb_score, df['text'], df['Genre'], cv=cv, scoring='accuracy'),\n",
    "        cross_val_score(bnb_score, df['text'], df['Genre'], cv=cv, scoring='f1_micro'),\n",
    "        cross_val_score(mnb_score, df['text'], df['Genre'], cv=cv, scoring='f1_micro'),\n",
    "    ]\n",
    "\n",
    "remove = get_list(mnb, bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Scenario 2: Retain all punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda text : preprocess_text(text, False))\n",
    "retain = get_list(mnb, bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(first, second, item1, item2, t):\n",
    "    first = {\n",
    "        \"BernoulliNB\": {\n",
    "            \"Macro F1\": first[0].mean(),\n",
    "            \"Micro F1\": first[4].mean(),\n",
    "            \"Accuracy\": first[2].mean()\n",
    "        },\n",
    "        \"MultinomialNB\": {\n",
    "            \"Macro F1\": first[1].mean(),\n",
    "            \"Micro F1\": first[5].mean(),\n",
    "            \"Accuracy\": first[3].mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    second = {\n",
    "        \"BernoulliNB\": {\n",
    "            \"Macro F1\": second[0].mean(),\n",
    "            \"Micro F1\": second[4].mean(),\n",
    "            \"Accuracy\": second[2].mean()\n",
    "        },\n",
    "        \"MultinomialNB\": {\n",
    "            \"Macro F1\": second[1].mean(),\n",
    "            \"Micro F1\": second[5].mean(),\n",
    "            \"Accuracy\": second[3].mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    table = []\n",
    "    for m, scores in first.items():\n",
    "        table.append([m, item1, \n",
    "                      f\"{scores['Macro F1']:.4f}\", \n",
    "                      f\"{scores['Micro F1']:.4f}\", \n",
    "                      f\"{scores['Accuracy']:.4f}\"])\n",
    "\n",
    "    for m, scores in second.items():\n",
    "        table.append([m, item2, \n",
    "                      f\"{scores['Macro F1']:.4f}\", \n",
    "                      f\"{scores['Micro F1']:.4f}\", \n",
    "                      f\"{scores['Accuracy']:.4f}\"])\n",
    "\n",
    "    headers = [\"Model\", t, \"Macro F1 Mean\", \"Micro F1 Mean\", \"Accuracy Mean\"]\n",
    "    \n",
    "    table.sort(key=lambda x: x[0])\n",
    "    \n",
    "    print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "\n",
    "print_scores(remove, retain, \"Remove Punctuation\", \"Retain Punctuation\", \"Punctuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "From the results, we can draw the following conclusions:\n",
    "\n",
    "- Retaining punctuation provided a slight improvement in the performance of **BNB**, as seen in the small increase in the mean scores and a slight reduction in the standard deviation.\n",
    "- For **MNB**, retaining punctuation did not significantly affect performance, as the differences in mean scores and standard deviations were minimal.\n",
    "\n",
    "Therefore, we will retain punctuation in the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Evaluation of Different Stop Words in NLP Models\n",
    "\n",
    "One of the primary differences between **NLTK** and **Scikit-learn** in handling stop words lies in the size and scope of their respective stop words lists. This divergence stems from the distinct goals and applications each library is designed to serve.\n",
    "\n",
    "Size and Scope\n",
    "- **NLTK**: **NLTK** provides a larger and more comprehensive list of stop words, including many common words that might not be relevant in certain contexts.\n",
    "- **Scikit-learn**: **Scikit-learn** offers a smaller, more curated list of stop words tailored for machine learning applications, focusing on words that are likely to be less informative for models.\n",
    "\n",
    "The choice of stop words list has a significant impact on the performance of NLP models. In this analysis, I will evaluate the performance of **MNB** and **BNB** models using two different stop words lists: **NLTK** and **Scikit-learn**. Similar to the previous tests, I will retain all punctuation and use *5-fold* *cross-validation* to compute the mean and standard deviation of **f1_macro**, **f1_micro**, and **accuracy** to assess the performance of the two stop words lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Scenario 1: NLTK Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda text : preprocess_text(text, nltk_stop_words))\n",
    "nltk_s = get_list(mnb, bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Scenario 2: Scikit-learn Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda text : preprocess_text(text, sklearn_stop_words))\n",
    "sklearn_s = get_list(mnb, bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(nltk_s, sklearn_s, \"NLTK Stop Words\", \"Scikit-learn Stop Words\", \"Stop Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "From the results, we can draw the following conclusions:\n",
    "\n",
    "The choice of stop words list (**NLTK** vs. **Scikit-learn**) has a marginal impact on the performance of **BNB** and almost no impact on **MNB**. Given the slight edge in Macro F1 and the negligible difference in other metrics, **BNB** with **NLTK** stop words can be considered marginally better. However, the overall difference is minimal, and both stop words lists are suitable for use with these models in **NLP** tasks.\n",
    "\n",
    "Therefore, we will use the **NLTK** stop words list in the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Evaluation of Top-N Words in NLP Models\n",
    "\n",
    "In this analysis, I will evaluate the impact of the number of top words (features) on the performance of **BNB** and **MNB** models. I choose the micro **F1 score** as the key score to evaluate, which can provide a comprehensive measure by accounting for each instance equally, ensuring a robust evaluation across all classes in the context of **Top-N** feature selection.\n",
    "\n",
    "I will vary the number of top words from $1$ to $50,000$ in increments of $1000$ and evaluate the performance of the models using *5-fold cross-validation*. The mean micro **F1 score** will be calculated for each model at different **Top-N** values to determine the optimal number of features for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanBNB = []\n",
    "meanMNB = []\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "r = range(1, 50000, 1000)\n",
    "\n",
    "for k in r:\n",
    "    bnb = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=k)), ('classifier', BernoulliNB())])\n",
    "    mnb = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=k)), ('classifier', MultinomialNB())])\n",
    "    \n",
    "    meanBNB.append(cross_val_score(bnb, df['text'], df['Genre'], cv=cv, scoring='f1_micro').mean())\n",
    "    meanMNB.append(cross_val_score(mnb, df['text'], df['Genre'], cv=cv, scoring='f1_micro').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(r, meanBNB, label='BNB')\n",
    "plt.plot(r, meanMNB, label='MNB')\n",
    "plt.xlabel('Max_Features')\n",
    "plt.ylabel('Mean F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "According to the plots, it appears that higher scores are achieved when the maximum features are between $1$ and $2000$. To get more precise metrics within this range, we will conduct an additional test with increments of $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanBNB = []\n",
    "meanMNB = []\n",
    "r = range(1, 2000, 100)\n",
    "\n",
    "for k in r:\n",
    "    bnb = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=k)), ('classifier', BernoulliNB())])\n",
    "    mnb = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=k)), ('classifier', MultinomialNB())])\n",
    "    \n",
    "    meanBNB.append(cross_val_score(bnb, df['text'], df['Genre'], cv=cv, scoring='f1_micro').mean())\n",
    "    meanMNB.append(cross_val_score(mnb, df['text'], df['Genre'], cv=cv, scoring='f1_micro').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(r, meanBNB, label='BNB')\n",
    "plt.plot(r, meanMNB, label='MNB')\n",
    "plt.xlabel('Max_Features')\n",
    "plt.ylabel('Mean F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Analysis of Optimal Max Features\n",
    "\n",
    "From the additional testing conducted with `max_features` ranging from $1$ to $2000$ in increments of $100$, we observe the following trends:\n",
    "\n",
    "- **Bernoulli Naive Bayes (BNB)**:\n",
    "  - The mean micro F1 score for BNB peaks at around 500 max features.\n",
    "  - Beyond 500 max features, the scores tend to gradually decline, indicating that including more features does not necessarily improve model performance and may even introduce noise.\n",
    "\n",
    "- **Multinomial Naive Bayes (MNB)**:\n",
    "  - The mean micro F1 score for MNB also performs well around 500 max features, although it is slightly lower than its highest score observed.\n",
    "  - This suggests that while MNB can potentially benefit from slightly more features, the performance gain is marginal beyond 500 features.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Given these observations, we can approximate that limiting the model to the top $500$ most frequent words is a reasonable and effective design choice. This balance helps maintain high model performance while avoiding the inclusion of excessive and potentially noisy features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Evaluating BNB and MNB Models\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=500)), ('classifier', BernoulliNB())])\n",
    "bnb_score_micro = cross_val_score(bnb, df['text'], df['Genre'], cv=cv, scoring='f1_micro')\n",
    "bnb_score_macro = cross_val_score(bnb, df['text'], df['Genre'], cv=cv, scoring='f1_macro')\n",
    "bnb_score_acc = cross_val_score(bnb, df['text'], df['Genre'], cv=cv, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=500)), ('classifier', MultinomialNB())])\n",
    "mnb_score_micro = cross_val_score(mnb, df['text'], df['Genre'], cv=cv, scoring='f1_micro')\n",
    "mnb_score_macro = cross_val_score(mnb, df['text'], df['Genre'], cv=cv, scoring='f1_macro')\n",
    "mnb_score_acc = cross_val_score(mnb, df['text'], df['Genre'], cv=cv, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(score1, score2, t):\n",
    "    plt.plot(score1, color='r', label='BNB')\n",
    "    plt.plot(score2, color='b', label='MNB')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Scode')\n",
    "    plt.title(t)\n",
    "    plt.axhline(y=score1.mean(), linestyle='--', color='r', label='BNB mean')\n",
    "    plt.axhline(y=score2.mean(), linestyle='--', color='b', label='MNB mean')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "print_plot(bnb_score_micro, mnb_score_micro, 'Comparison of BNB and MNB Models on F1 Micro Score')\n",
    "plt.subplot(2, 2, 2)\n",
    "print_plot(bnb_score_macro, mnb_score_macro, 'Comparison of BNB and MNB Models on F1 Macro Score')\n",
    "plt.subplot(2, 2, 3)\n",
    "print_plot(bnb_score_acc, mnb_score_acc, 'Comparison of BNB and MNB Models on Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Based on the plots and the data in the table, we can observe that Bernoulli Naive Bayes (**BNB**) scores are generally higher than those of Multinomial Naive Bayes (**MNB**) across various metrics, both for most subsets and on average.\n",
    "\n",
    "Hypothesis on **BNB** vs. **MNB** Performance\n",
    "\n",
    "- Bernoulli Naive Bayes:   \n",
    "This model works with binary features, meaning it only considers whether a word is present in a document, not how often it appears.  This binary approach is particularly effective when the mere presence of certain keywords strongly indicates the class, regardless of their frequency.\n",
    "- Multinomial Naive Bayes:   \n",
    "This model relies on the frequency of words.  While this can be beneficial when the frequency of terms is a strong class indicator, it might be less effective when the presence of terms is more important than their frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### New Model: Gradient Boosting Machine (GBM)\n",
    "\n",
    "Advantages:\n",
    "- It Often results in better performance compared to random forests and individual decision trees.\n",
    "- Effective in handling various types of data including categorical and numerical.\n",
    "\n",
    "When working with Gradient Boosting Machines (**GBM**), one crucial hyperparameter to tune is the number of boosting stages, specified by n_estimators. Increasing n_estimators generally enhances model performance by allowing the model to learn more complex patterns through additional combinations. However, too many boosting stages can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "To find an optimal value for n_estimators, we can evaluate the model’s performance (using metrics such as the **micro F1 score**) across a range of boosting stages from $1$ to $201$ in increments of $20$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_list = []\n",
    "\n",
    "for i in range(1, 201, 20):\n",
    "    gbm = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=500)), ('classifier', GradientBoostingClassifier(n_estimators=i))])\n",
    "    estimator_list.append(cross_val_score(gbm, df['text'], df['Genre'], cv=cv, scoring='f1_micro').mean())\n",
    "    \n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(range(1, 201, 20), estimator_list, label='GBM')\n",
    "plt.xlabel('Estimators')\n",
    "plt.ylabel('Mean F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Based on the detailed analysis and incremental tests, it is observed that the Gradient Boosting Machine (**GBM**) achieves optimal performance when n_estimators are set $60$. Within this range, the scores remain consistently high, and beyond this range, the performance starts to decline. Therefore, we will select the Median value `n_estimators=60` as the hyperparameter for **GBM** and compare its performance with **BNB** and **MNB**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = Pipeline(steps=[('preprocessor', CountVectorizer(max_features=500)), ('classifier', GradientBoostingClassifier(n_estimators=60))])\n",
    "gbm_score_micro = cross_val_score(gbm, df['text'], df['Genre'], cv=cv, scoring='f1_micro')\n",
    "gbm_score_macro = cross_val_score(gbm, df['text'], df['Genre'], cv=cv, scoring='f1_macro')\n",
    "gbm_score_acc = cross_val_score(gbm, df['text'], df['Genre'], cv=cv, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(score1, score2, score3, t):\n",
    "    plt.plot(score1, color='r', label='BNB')\n",
    "    plt.plot(score2, color='b', label='MNB')\n",
    "    plt.plot(score3, color='g', label='GBM')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Scode')\n",
    "    plt.title(t)\n",
    "    plt.axhline(y=score1.mean(), linestyle='--', color='r', label='BNB mean')\n",
    "    plt.axhline(y=score2.mean(), linestyle='--', color='b', label='MNB mean')\n",
    "    plt.axhline(y=score3.mean(), linestyle='--', color='g', label='GBM mean')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "print_plot(bnb_score_micro, mnb_score_micro, gbm_score_micro, 'Comparison of GBM, BNB and MNB Models on F1 Micro Score')\n",
    "plt.subplot(2, 2, 2)\n",
    "print_plot(bnb_score_macro, mnb_score_macro, gbm_score_macro, 'Comparison of GBM, BNB and MNB Models on F1 Macro Score')\n",
    "plt.subplot(2, 2, 3)\n",
    "print_plot(bnb_score_acc, mnb_score_acc, gbm_score_acc, 'Comparison of GBM, BNB and MNB Models on Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The results indicate that the **GBM** model scores are significantly higher than both the **MNB** and **BNB** models.\n",
    "\n",
    "Hypothesis on the **GBM** Excellent Performance \n",
    "- Boosting Mechanism:   \n",
    "**GBM** builds models sequentially, where each new model attempts to correct the errors of the previous ones.  This iterative approach allows **GBM** to handle complex patterns in the data more effectively than a single model.\n",
    "- Combination of Weak Learners:   \n",
    "By combining multiple weak learners, **GBM** creates a strong predictive model that reduces both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Recommendation Methods\n",
    "### Data Preprocessing\n",
    "Use **NLTK** stop words and retain all punctuation. Preprocess the text data by converting it to lowercase, removing stop words, and stemming the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "nltk_stop_words = [set(stopwords.words('english'))]\n",
    "\n",
    "# Load the dataset\n",
    "dictionary = dict()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train = []\n",
    "valid = []\n",
    "for file in os.listdir('dataset'):\n",
    "    if file.endswith('tsv'):\n",
    "        content = pd.read_csv(os.path.join('dataset', file), sep='\\t')\n",
    "        train.append(content.iloc[:150])\n",
    "        valid.append(content.iloc[150:])\n",
    "\n",
    "train = pd.concat(train, ignore_index=True)\n",
    "valid = pd.concat(valid, ignore_index=True)\n",
    "\n",
    "train['text'] = train['Title'].astype(str) + ' ' + train['Release Year'].astype(str) + ' ' + train['Genre'].astype(str) + ' ' + train['Director'].astype(str) + ' ' + train['Cast'].astype(str) + ' ' + train['Plot'].astype(str) + ' ' + train['Origin/Ethnicity'].astype(str)\n",
    "\n",
    "valid['text'] = valid['Title'].astype(str) + ' ' + valid['Release Year'].astype(str) + ' ' + valid['Genre'].astype(str) + ' ' + valid['Director'].astype(str) + ' ' + valid['Cast'].astype(str) + ' ' + valid['Plot'].astype(str) + ' ' + valid['Origin/Ethnicity'].astype(str)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s\\'.,!?@#&$%\\-+*/=]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in nltk_stop_words]\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train['text'] = train['text'].apply(preprocess_text)\n",
    "valid['text'] = valid['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "X_test = vectorizer.transform(valid['text'])\n",
    "\n",
    "train['Vector'] = list(X_train)\n",
    "valid['Vector'] = list(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Building Model (Gradient Boosting Classifier)\n",
    "\n",
    "Use the hyperparameter which calculated in the previous section.\n",
    "`n_estimators = 60` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=60\n",
    "model = Pipeline(steps=[('preprocessor', vectorizer), ('classifier', GradientBoostingClassifier(n_estimators=n))])\n",
    "model.fit(train['text'], train['Genre'])\n",
    "train['pred'] = model.predict(train['text'])\n",
    "valid['pred'] = model.predict(valid['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Processing the User Data\n",
    "\n",
    "This code constructs a user profile by generating a dictionary that contains the term frequency vectors for each category the user likes. Specifically, the code filters documents in each category that match the user’s keywords, calculates the average TF-IDF vector for these relevant documents, and stores these average vectors in the user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_profile(user, data):\n",
    "    user_profile = {}\n",
    "    for _, dframe in user.iterrows():\n",
    "        genre_docs = data[data['pred'] == dframe['Genre']]\n",
    "        if genre_docs.empty:\n",
    "            continue\n",
    "        relevant_docs = genre_docs[genre_docs['text'].apply(lambda x: any(word in dframe['Keywords'].split() for word in x.split()))]\n",
    "        if relevant_docs.empty:\n",
    "            user_profile[dframe['Genre']] = np.zeros(vectorizer.transform(['']).shape[1])\n",
    "        else:\n",
    "            vectors = np.array([vec.toarray().flatten() for vec in relevant_docs['Vector']])\n",
    "            user_profile[dframe['Genre']] = np.mean(vectors, axis=0)\n",
    "            \n",
    "    return user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = pd.read_csv(os.path.join('./', 'user1.tsv'), sep='\\t', header=None, names=['Genre', 'Keywords'])\n",
    "user2 = pd.read_csv(os.path.join('./', 'user2.tsv'), sep='\\t', header=None, names=['Genre', 'Keywords'])\n",
    "\n",
    "user1_profile = create_user_profile(user1, train)\n",
    "user2_profile = create_user_profile(user2, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "user3_keywords = {\n",
    "    'comedy': 'funny laugh joke humor',\n",
    "    'drama': 'Ghastly Horrifying Creepy Haunted ghastly',\n",
    "    'horror': 'ghost scary horror fear',\n",
    "    'romance': 'romantic love kiss relationship',\n",
    "    'sci-Fi': 'space future technology alien',\n",
    "    'thriller': 'suspense mystery crime detective',\n",
    "}\n",
    "\n",
    "user3_keywords_df = pd.DataFrame(list(user3_keywords.items()), columns=['Genre', 'Keywords'])\n",
    "user3_profile = create_user_profile(user3_keywords_df, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(vec, tfidf_vector, top=20):\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    sorted_items = np.argsort(tfidf_vector)[::-1]\n",
    "    return [(feature_names[j], tfidf_vector[j]) for j in sorted_items[:top]]\n",
    "\n",
    "\n",
    "def print_top_words(a, index):\n",
    "    print(f\"User {index} Top 20 Word\")\n",
    "    for g in a:\n",
    "        top_words = get_top_words(vectorizer, a[g])\n",
    "        print(f\"Genre: {g}\\n\\t\", end='')\n",
    "        print(', '.join([f\"{word}\" for word, _ in top_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(user1_profile, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(user2_profile, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(user3_profile, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### Conclusion of Top 20 Words\n",
    "Upon examining the top 20 words, I find them relatively reasonable. Firstly, keywords related to the themes are present in the results. There are also words like “hi,” which appear prominently across multiple themes. After associating these words with their respective themes, I believe they are correctly included.\n",
    "\n",
    "Validating the System with User 3\n",
    "\n",
    "I deliberately added some words to User 3’s profile that are either unrelated or even contradictory to the themes. (For drama movie, I add five contradictory words) These words did not appear in the results, which I find appropriate. It indicates that the system does not overly prioritize words just because they are in the user’s interest list, ensuring the relevance of the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Choosing and Justifying Metrics\n",
    "\n",
    "To evaluate the performance of the recommendation method, we should consider the following metrics:\n",
    "\n",
    "1.\tPrecision@N: Measures the proportion of recommended movies in the top N that the user actually likes. \n",
    "2.\tRecall@N: Measures the proportion of all movies that the user likes which are included in the top N recommendations. \n",
    "3.\tF1-Score@N: The harmonic mean of Precision@N and Recall@N. This gives a balanced measure of both precision and recall.\n",
    "4.\tHit Rate: The proportion of users who liked at least one movie in the top N recommendations. \n",
    "5.\tDiversity: Measures the variety of genres or types of movies in the recommendations. \n",
    "\n",
    "Choosing the Value of N\n",
    "\n",
    "N should be chosen based on how the movies are presented to the user. Considering the variety of movies and the need to get useful feedback, we should choose a value for N that balances between providing enough options for the user to like some movies, but not overwhelming them. A reasonable choice might be to show N = 10 movies per genre. This gives the user a diverse set of options while still keeping the list manageable.\n",
    "\n",
    "Evaluation Method\n",
    "\n",
    "We will evaluate the performance of the recommendation method by testing how well the top N movies suggested for Week 4 match the interests of each user. Assume each user likes all and only those movies in the top N recommendations that matched their profile for the predicted genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_profile, test_data, vec, N=10):\n",
    "    all_recommendations = []\n",
    "\n",
    "    for genre, profile_vector in user_profile.items():\n",
    "        genre_movies = test_data[test_data['pred'] == genre]\n",
    "        if not genre_movies.empty:\n",
    "            genre_tfidf = vec.transform(genre_movies['text'])\n",
    "            similarities = cosine_similarity(profile_vector.reshape(1, -1), genre_tfidf).flatten()\n",
    "            sorted_similarities = np.argsort(similarities)[::-1]\n",
    "            genre_movies = genre_movies.iloc[sorted_similarities]\n",
    "            all_recommendations.append(genre_movies)\n",
    "\n",
    "    # Combine all genre recommendations\n",
    "    combined_recommendations = pd.concat(all_recommendations)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_recommendations = combined_recommendations.drop_duplicates(subset=['Title'])\n",
    "    \n",
    "    # Sort by similarity score (descending order) and select top N\n",
    "    combined_recommendations['similarity'] = np.concatenate([cosine_similarity(user_profile[genre].reshape(1, -1), vec.transform(combined_recommendations[combined_recommendations['pred'] == genre]['text'])).flatten() for genre in user_profile])\n",
    "    top_n_recommendations = combined_recommendations.sort_values(by='similarity', ascending=False).head(N)\n",
    "\n",
    "    return top_n_recommendations\n",
    "\n",
    "user1_recommendations = recommend_movies(user1_profile, valid, vectorizer)\n",
    "user2_recommendations = recommend_movies(user2_profile, valid, vectorizer)\n",
    "user3_recommendations = recommend_movies(user3_profile, valid, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations(recommendations):\n",
    "    for _, row in recommendations.iterrows():\n",
    "        print(f\"\\t({row['Genre']}) {row['Title']} ,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User 1 Recommendations:\")\n",
    "print_recommendations(user1_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User 2 Recommendations:\")\n",
    "print_recommendations(user2_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User 3 Recommendations:\")\n",
    "print_recommendations(user3_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "By evaluating the performance of a movie recommendation system by calculating precision, recall, F1-score, and hit rate. This evaluation is based on user feedback simulated using randomly generated ‘liked’ labels for the movies in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['liked'] = np.random.choice([0, 1], size=len(valid))\n",
    "\n",
    "def evaluate_recommendations(user_profile, va, vec, N=10):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    hit_rate_count = 0\n",
    "    \n",
    "    for genre, profile_vector in user_profile.items():\n",
    "        genre_movies = va[va['pred'] == genre]\n",
    "        if genre_movies.empty:\n",
    "            continue\n",
    "        \n",
    "        genre_tfidf = vec.transform(genre_movies['text'])\n",
    "        similarities = cosine_similarity(profile_vector.reshape(1, -1), genre_tfidf).flatten()\n",
    "        top_n_indices = similarities.argsort()[-N:][::-1]\n",
    "        top_n_movies = genre_movies.iloc[top_n_indices]\n",
    "        \n",
    "        liked_movies = top_n_movies[top_n_movies['liked'] == 1]\n",
    "        \n",
    "        precision = len(liked_movies) / N\n",
    "        recall = len(liked_movies) / len(genre_movies[genre_movies['liked'] == 1])\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        if not liked_movies.empty:\n",
    "            hit_rate_count += 1\n",
    "    \n",
    "    precision_avg = np.mean(precision_scores)\n",
    "    recall_avg = np.mean(recall_scores)\n",
    "    f1_avg = np.mean(f1_scores)\n",
    "    hit_rate = hit_rate_count / len(user_profile)\n",
    "    \n",
    "    return precision_avg, recall_avg, f1_avg, hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision1, recall1, f1_1, hit_rate1 = evaluate_recommendations(user1_profile, valid, vectorizer, N=10)\n",
    "precision2, recall2, f1_2, hit_rate2 = evaluate_recommendations(user2_profile, valid, vectorizer, N=10)\n",
    "precision3, recall3, f1_3, hit_rate3 = evaluate_recommendations(user3_profile, valid, vectorizer, N=10)\n",
    "\n",
    "print(f\"User 1 - Precision@10: {precision1}, Recall@10: {recall1}, F1-Score@10: {f1_1}, Hit Rate: {hit_rate1}\")\n",
    "print(f\"User 2 - Precision@10: {precision2}, Recall@10: {recall2}, F1-Score@10: {f1_2}, Hit Rate: {hit_rate2}\")\n",
    "print(f\"User 3 - Precision@10: {precision3}, Recall@10: {recall3}, F1-Score@10: {f1_3}, Hit Rate: {hit_rate3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Analysis of Recommendation Performance\n",
    "\n",
    "According to the results, we can analyze the performance of the recommendation method for each user based on the following metrics:\n",
    "1. **Precision@10**: \n",
    "   - This metric indicates the proportion of recommended movies in the top 10 that the users actually liked.\n",
    "   - User 1 has a precision of 0.48, meaning 48% of the recommended movies were liked by the user.\n",
    "   - User 2 has a precision of 0.5, meaning 50% of the recommended movies were liked by the user.\n",
    "   - User 3 has a precision of 0.58, meaning 58% of the recommended movies were liked by the user.\n",
    "\n",
    "2. **Recall@10**: \n",
    "   - This metric indicates the proportion of all movies that the users liked which are included in the top 10 recommendations.\n",
    "   - Recall values for all users are relatively low, indicating that a small fraction of the movies they liked was captured in the top 10 recommendations.\n",
    "\n",
    "3. **F1-Score@10**: \n",
    "   - The F1-Score is the harmonic mean of precision and recall, providing a balance between the two.\n",
    "   - The F1-Scores are low for all users, reflecting the low recall despite the higher precision values.\n",
    "\n",
    "4. **Hit Rate**: \n",
    "   - This metric indicates the proportion of users who liked at least one movie in the top 10 recommendations.\n",
    "   - The hit rate is 1.0 for all users, indicating that every user liked at least one of the recommended movies, which is a positive sign.\n",
    "\n",
    "\n",
    "#### Comparison and Analysis of User Data\n",
    "\n",
    "1. **User 1 and User 3**:\n",
    "   - **Data Breadth**: Both User 1 and User 3 provided a wide range of genres and lots of keywords for each genre.\n",
    "   - **Keyword Coverage**: The extensive keywords increased the likelihood of matching words in the top 20 lists, making it easier to infer the user's preferences accurately.\n",
    "   - **Resilience to Noise**: Despite the presence of misleading keywords in User 3's drama category, the other relevant keywords still allowed the system to infer the user's preferences effectively.\n",
    "   - **Impact on Recommendations**: The broader range of data and keywords allowed the recommendation system to hit more of the user’s liked movies, resulting in higher precision and recall.\n",
    "\n",
    "2. **User 2**:\n",
    "   - **Limited Scope**: User 2 provided very few genres and keywords, which restricted the data available for generating recommendations.\n",
    "   - **Keyword Scarcity**: The limited number of keywords made it difficult for the system to accurately identify user preferences, as higher frequency words in other genres could easily overshadow the sparse keywords provided.\n",
    "   - **Hit Rate**: Despite the lower precision and recall, the system was still able to recommend at least one movie that matched the user's preferences, resulting in a hit rate of 1.0.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- The **precision** values indicate that the recommendations are relatively accurate, with more than 43% of the recommended movies being liked by the users.\n",
    "- The **recall** values are low, suggesting that the recommendations do not cover a significant portion of all the movies that users liked.\n",
    "- The **F1-Scores** reflect the imbalance between precision and recall, highlighting the need to improve the coverage of the recommendations.\n",
    "- The **hit rate** being 1.0 means that users are finding at least one movie they like in the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Part 3. User Evaluation\n",
    "\n",
    "I will randomly select 200 entries from train_data, divide them into four groups (50 entries each), and ensure each group contains different genres. Then, we will generate a table with all movie information for your friend to evaluate and record which movies they like or dislike.\n",
    "\n",
    "Randomly Select 200 Entries from train_data, Divide into Four Groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "w1 = shuffled_train.iloc[:50]\n",
    "w2 = shuffled_train.iloc[50:100]\n",
    "w3 = shuffled_train.iloc[100:150]\n",
    "w4 = shuffled_train.iloc[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1['Title'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2['Title'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w3['Title'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "My friend evaluates each movie and marks it as liked (1) or disliked (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_p = {\n",
    "    'Barrens, TheThe Barrens': 0, 'Santa Claus Conquers the Martians': 1, 'Brake': 0, \"Perrier's Bounty\": 1, 'All That Heaven Allows':0\n",
    "}\n",
    "\n",
    "w2_p = {\n",
    "    'Where the Wild Things Are':1, 'The Next Three Days':1, 'Somewhere Slow':0, ' The Scarecrow':1, 'Case Closed: Captured in Her Eyes':0\n",
    "}\n",
    "\n",
    "w3_p = {\n",
    "    'Crazy !C.R.A.Z.Y.': 1, 'Piranha 3DD': 0, 'Circle': 1, 'Pink and Gray': 0, 'The Secret Life of Pets': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([w1, w2, w3])\n",
    "train_data['liked'] = 0\n",
    "\n",
    "for title, rating in w1_p.items():\n",
    "    train_data.loc[train_data['Title'] == title, 'liked'] = rating\n",
    "for title, rating in w2_p.items():\n",
    "    train_data.loc[train_data['Title'] == title, 'liked'] = rating\n",
    "for title, rating in w3_p.items():\n",
    "    train_data.loc[train_data['Title'] == title, 'liked'] = rating\n",
    "    \n",
    "train_data['liked'] = train_data['liked'].astype(int)\n",
    "\n",
    "train_data['text'] = train_data['Title'].astype(str) + ' ' + train_data['Genre'].astype(str) + ' ' + train_data['Director'].astype(str) + ' ' + train_data['Cast'].astype(str) + ' ' + train_data['Plot'].astype(str) + ' ' + train_data['Origin/Ethnicity'].astype(str)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "\n",
    "week4_data = w4.copy()\n",
    "week4_data['text'] = week4_data['Title'].astype(str) + ' ' + week4_data['Genre'].astype(str) + ' ' + week4_data['Director'].astype(str) + ' ' + week4_data['Cast'].astype(str) + ' ' + week4_data['Plot'].astype(str) + ' ' + week4_data['Origin/Ethnicity'].astype(str)\n",
    "week4_data['text'] = week4_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('classifier', GradientBoostingClassifier(n_estimators=60))])\n",
    "model.fit(X_train, train_data['liked'])\n",
    "X_week4 = vectorizer.transform(week4_data['text'])\n",
    "\n",
    "week4_predictions = model.predict(X_week4)\n",
    "week4_data['predicted_liked'] = week4_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(recommendations, N=10):\n",
    "    top_n_recommendations = recommendations.nlargest(N, 'predicted_liked')\n",
    "    print(\"Top N Recommendations:\")\n",
    "    for j, row in top_n_recommendations.iterrows():\n",
    "        print(f\"\\t({row['Genre']}) {row['Title']}\")\n",
    "\n",
    "evaluate_recommendations(week4_data, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "| Information                             | Love or not |\n",
    "|-----------------------------------------|-------------|\n",
    "| (romance) All That Heaven Allows        | Yes         |\n",
    "| (sci-fi) War for the Planet of the Apes | No          |\n",
    "| (drama) Call Me by Your Name            | No          |\n",
    "| (horror) Nurse 3D                       | No          |\n",
    "| (romance) Loving Couples                | Yes         |\n",
    "| (comedy) House of Wolves                | No          |\n",
    "| (romance) Picture Perfect               | Yes         |\n",
    "| (sci-fi) Looker                         | No          |\n",
    "| (sci-fi) The Space Between Us           | Yes         |\n",
    "| (drama) Red Amnesia                     | Yes         |\n",
    "\n",
    "Despite this initial 50–50 accuracy rate, it is important to consider several factors:\n",
    "\n",
    "- Subjective Bias: The friend’s personal biases and subjective opinions might have influenced the evaluations, which is an expected variability in human preferences.\n",
    "- Sample Size: The sample size of 200 movies is a good starting point, but larger datasets would provide more robust training and more accurate predictions.\n",
    "- Model Training: With a large enough amount of data, the GBM model can indeed predict people’s preferences or inclinations. The larger and more diverse the training data, the better the model can generalize and make accurate predictions.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The study highlights the potential of the GBM model in predicting user preferences, even with a relatively small dataset and the inherent subjectivity of human evaluators. By expanding the dataset and including more diverse user evaluations, the model’s accuracy can be significantly improved. This suggests that GBM, with enough data, can be a powerful tool in recommendation systems, capable of predicting user likes and dislikes with considerable accuracy.​⬤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
