{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4GuH-MSj2yOD"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/ymoslem/OpenNMT-Tutorial/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxxNn8tg3EGL",
        "outputId": "41151dfa-1b71-4af8-f6a0-9594af52ec60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt\n",
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (268/268), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 268 (delta 133), reused 189 (delta 97), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (268/268), 69.06 KiB | 4.60 MiB/s, done.\n",
            "Resolving deltas: 100% (133/133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r MT-Preparation/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRry_jc23HpL",
        "outputId": "38cb0ef4-38df-4e42-8f9e-a03aaaec7b59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
        "!unzip en-fr.txt.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LZddSvU3N9B",
        "outputId": "90c1b85d-b300-442e-ff23-ba80bfbddf93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-28 21:00:43--  https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10014972 (9.6M) [application/zip]\n",
            "Saving to: ‘en-fr.txt.zip’\n",
            "\n",
            "en-fr.txt.zip       100%[===================>]   9.55M  23.7MB/s    in 0.4s    \n",
            "\n",
            "2024-03-28 21:00:44 (23.7 MB/s) - ‘en-fr.txt.zip’ saved [10014972/10014972]\n",
            "\n",
            "Archive:  en-fr.txt.zip\n",
            "  inflating: UN.en-fr.en             \n",
            "  inflating: UN.en-fr.fr             \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/filtering/filter.py UN.en-fr.fr UN.en-fr.en fr en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtAV1lhe3ak7",
        "outputId": "c6d7a12f-a96f-4b9e-cde7-2f34808bda45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape (rows, columns): (74067, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 74067\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 60662\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 60476\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 59719\n",
            "--- HTML Removed\t\t\t--> Rows: 59719\n",
            "--- Rows will remain in true-cased\t--> Rows: 59719\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 59719\n",
            "--- Rows Shuffled\t\t\t--> Rows: 59719\n",
            "--- Source Saved: UN.en-fr.fr-filtered.fr\n",
            "--- Target Saved: UN.en-fr.en-filtered.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls MT-Preparation/subwording/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1l3SSEB3fhf",
        "outputId": "5a7c6abd-9793-41ed-8990-aec0be2d3010"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/1-train_unigram.py UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6xEmpHo3hwD",
        "outputId": "5e7a8f50-0c7f-4786-a7ea-22e5c40f77aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=UN.en-fr.fr-filtered.fr --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: UN.en-fr.fr-filtered.fr\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: UN.en-fr.fr-filtered.fr\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 59719 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=19614832\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9546% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=82\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999546\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 59719 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=13630419\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 61805 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 59719\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 48938\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 48938 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23065 obj=12.0957 num_tokens=205556 num_tokens/piece=8.91203\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16778 obj=8.81693 num_tokens=205778 num_tokens/piece=12.2648\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n",
            "Done, training a SentencepPiece model for the Source finished successfully!\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=UN.en-fr.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: UN.en-fr.en-filtered.en\n",
            "  input_format: \n",
            "  model_prefix: target\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: UN.en-fr.en-filtered.en\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 59719 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=17772658\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9623% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=70\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999623\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 59719 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=11807312\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 46525 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 59719\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 44916\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 44916 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19499 obj=11.8498 num_tokens=207239 num_tokens/piece=10.6282\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13744 obj=8.60033 num_tokens=207645 num_tokens/piece=15.108\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n",
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEEN06oB3qe9",
        "outputId": "da20d26b-458f-40f3-a015-264c6d760b72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: UN.en-fr.fr-filtered.fr\n",
            "Target Dataset: UN.en-fr.en-filtered.en\n",
            "Done subwording the source file! Output: UN.en-fr.fr-filtered.fr.subword\n",
            "Done subwording the target file! Output: UN.en-fr.en-filtered.en.subword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 UN.en-fr.fr-filtered.fr.subword UN.en-fr.en-filtered.en.subword\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_dRM3ui3wXP",
        "outputId": "e9f8c874-d86b-442e-c9d2-7c8b2fed845d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (59719, 2)\n",
            "--- Empty Cells Deleted --> Rows: 59719\n",
            "--- Wrote Files\n",
            "Done!\n",
            "Output files\n",
            "UN.en-fr.fr-filtered.fr.subword.train\n",
            "UN.en-fr.en-filtered.en.subword.train\n",
            "UN.en-fr.fr-filtered.fr.subword.dev\n",
            "UN.en-fr.en-filtered.en.subword.dev\n",
            "UN.en-fr.fr-filtered.fr.subword.test\n",
            "UN.en-fr.en-filtered.en.subword.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/nmt/ /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "sHzRhFm3306x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install OpenNMT-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W70Uqvd94Dty",
        "outputId": "a2d7af51-ff62-4c29-827d-8d434a997654"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m256.0/262.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.1+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<5,>=4 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-4.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.37 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.6.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, nvidia-cusolver-cu12, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.5.1 colorama-0.4.6 configargparse-1.7 ctranslate2-4.1.0 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 portalocker-2.8.2 pyahocorasick-2.1.0 pybind11-2.12.0 pyonmttok-1.37.1 rapidfuzz-3.7.0 sacrebleu-2.4.1 waitress-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
        "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
        "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.fren\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "wyMSJ9SD4TLW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc --all\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54YpA4-94ZOd",
        "outputId": "424b1d3f-86ab-41f8-ca18-e8b569c98aa5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oSxAoJd4jR1",
        "outputId": "7108e449-d40c-46d8-dfb9-b7aef19ccad0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-03-28 21:06:03,965 INFO] Counter vocab from -1 samples.\n",
            "[2024-03-28 21:06:03,965 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-03-28 21:06:09,662 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2104)\n",
            "\n",
            "[2024-03-28 21:06:09,845 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2089)\n",
            "\n",
            "[2024-03-28 21:06:09,914 INFO] Counters src: 14704\n",
            "[2024-03-28 21:06:09,915 INFO] Counters tgt: 11886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTw9ZuIM4mMq",
        "outputId": "a46dbdc4-8bf2-4e29-bb33-c90d373b8511"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-e93cc325-d4ec-0151-3e07-06489922df1c)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV0gVFCg4sgl",
        "outputId": "4643bb47-e721-4839-9872-0ef04f508f0c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-03-28 21:06:38,890 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-03-28 21:06:38,890 INFO] Parsed 2 corpora from -data.\n",
            "[2024-03-28 21:06:38,890 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-03-28 21:06:38,984 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁de', ',', \"'\", '▁et', '▁', '▁la']\n",
            "[2024-03-28 21:06:38,984 INFO] The decoder start token is: <s>\n",
            "[2024-03-28 21:06:38,984 INFO] Building model...\n",
            "[2024-03-28 21:06:40,954 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-03-28 21:06:40,954 INFO] Non quantized layer compute is fp16\n",
            "[2024-03-28 21:06:41,176 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(14712, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(11896, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=11896, bias=True)\n",
            ")\n",
            "[2024-03-28 21:06:41,179 INFO] encoder: 26420224\n",
            "[2024-03-28 21:06:41,179 INFO] decoder: 37378680\n",
            "[2024-03-28 21:06:41,179 INFO] * number of parameters: 63798904\n",
            "[2024-03-28 21:06:41,180 INFO] Trainable parameters = {'torch.float32': 63798904, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-03-28 21:06:41,180 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-03-28 21:06:41,180 INFO]  * src vocab size = 14712\n",
            "[2024-03-28 21:06:41,180 INFO]  * tgt vocab size = 11896\n",
            "[2024-03-28 21:06:41,602 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-03-28 21:06:41,603 INFO] Starting training on GPU: [0]\n",
            "[2024-03-28 21:06:41,603 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-03-28 21:06:41,603 INFO] Scoring with: ['filtertoolong']\n",
            "[2024-03-28 21:06:47,566 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-03-28 21:06:55,935 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-03-28 21:07:02,268 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-03-28 21:07:12,262 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-03-28 21:08:11,670 INFO] Step 100/ 3000; acc: 8.6; ppl: 1604.3; xent: 7.4; lr: 0.00028; sents:   27788; bsz: 3876/3409/69; 17214/15138 tok/s;     90 sec;\n",
            "[2024-03-28 21:08:40,041 INFO] Step 200/ 3000; acc: 30.0; ppl: 156.8; xent: 5.1; lr: 0.00056; sents:   29277; bsz: 3886/3460/73; 54783/48787 tok/s;    118 sec;\n",
            "[2024-03-28 21:08:53,475 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19691)\n",
            "\n",
            "[2024-03-28 21:08:53,475 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-03-28 21:08:58,176 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2024-03-28 21:09:08,027 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2024-03-28 21:09:12,963 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2024-03-28 21:09:25,127 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2024-03-28 21:10:12,955 INFO] Step 300/ 3000; acc: 41.3; ppl:  62.9; xent: 4.1; lr: 0.00084; sents:   27734; bsz: 3889/3459/69; 16741/14893 tok/s;    211 sec;\n",
            "[2024-03-28 21:10:42,949 INFO] Step 400/ 3000; acc: 47.1; ppl:  41.6; xent: 3.7; lr: 0.00112; sents:   27746; bsz: 3888/3415/69; 51853/45541 tok/s;    241 sec;\n",
            "[2024-03-28 21:11:11,187 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19687)\n",
            "\n",
            "[2024-03-28 21:11:11,187 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2024-03-28 21:11:21,315 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2024-03-28 21:11:26,019 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2024-03-28 21:11:37,608 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2024-03-28 21:11:43,347 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2024-03-28 21:12:22,557 INFO] Step 500/ 3000; acc: 53.7; ppl:  28.1; xent: 3.3; lr: 0.00140; sents:   29448; bsz: 3885/3453/74; 15601/13865 tok/s;    341 sec;\n",
            "[2024-03-28 21:12:52,532 INFO] Step 600/ 3000; acc: 60.8; ppl:  19.8; xent: 3.0; lr: 0.00168; sents:   28340; bsz: 3884/3450/71; 51827/46046 tok/s;    371 sec;\n",
            "[2024-03-28 21:13:22,228 INFO] Step 700/ 3000; acc: 64.2; ppl:  16.9; xent: 2.8; lr: 0.00196; sents:   28779; bsz: 3899/3434/72; 52520/46250 tok/s;    401 sec;\n",
            "[2024-03-28 21:13:33,741 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19787)\n",
            "\n",
            "[2024-03-28 21:13:33,742 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2024-03-28 21:13:44,637 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2024-03-28 21:13:49,384 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2024-03-28 21:14:02,135 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2024-03-28 21:15:03,636 INFO] Step 800/ 3000; acc: 68.5; ppl:  13.5; xent: 2.6; lr: 0.00224; sents:   28668; bsz: 3894/3465/72; 15361/13668 tok/s;    502 sec;\n",
            "[2024-03-28 21:15:33,958 INFO] Step 900/ 3000; acc: 67.7; ppl:  13.9; xent: 2.6; lr: 0.00252; sents:   29730; bsz: 3867/3435/74; 51020/45315 tok/s;    532 sec;\n",
            "[2024-03-28 21:15:53,460 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19726)\n",
            "\n",
            "[2024-03-28 21:15:53,461 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 20\n",
            "[2024-03-28 21:15:58,357 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 21\n",
            "[2024-03-28 21:16:05,032 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 22\n",
            "[2024-03-28 21:16:16,165 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 23\n",
            "[2024-03-28 21:16:21,904 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 24\n",
            "[2024-03-28 21:17:10,823 INFO] Step 1000/ 3000; acc: 69.2; ppl:  12.9; xent: 2.6; lr: 0.00279; sents:   26345; bsz: 3893/3429/66; 16076/14160 tok/s;    629 sec;\n",
            "[2024-03-28 21:17:13,096 INFO] valid stats calculation\n",
            "                           took: 2.2710249423980713 s.\n",
            "[2024-03-28 21:17:13,100 INFO] Train perplexity: 41.3481\n",
            "[2024-03-28 21:17:13,100 INFO] Train accuracy: 51.1407\n",
            "[2024-03-28 21:17:13,100 INFO] Sentences processed: 283855\n",
            "[2024-03-28 21:17:13,100 INFO] Average bsz: 3886/3441/71\n",
            "[2024-03-28 21:17:13,100 INFO] Validation perplexity: 13.7812\n",
            "[2024-03-28 21:17:13,101 INFO] Validation accuracy: 69.2707\n",
            "[2024-03-28 21:17:13,101 INFO] Model is improving ppl: inf --> 13.7812.\n",
            "[2024-03-28 21:17:13,101 INFO] Model is improving acc: -inf --> 69.2707.\n",
            "[2024-03-28 21:17:13,112 INFO] Saving checkpoint models/model.fren_step_1000.pt\n",
            "[2024-03-28 21:17:47,697 INFO] Step 1100/ 3000; acc: 70.9; ppl:  11.8; xent: 2.5; lr: 0.00266; sents:   29087; bsz: 3893/3443/73; 42231/37347 tok/s;    666 sec;\n",
            "[2024-03-28 21:18:17,407 INFO] Step 1200/ 3000; acc: 71.6; ppl:  11.5; xent: 2.4; lr: 0.00255; sents:   28098; bsz: 3879/3429/70; 52228/46161 tok/s;    696 sec;\n",
            "[2024-03-28 21:18:21,057 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19679)\n",
            "\n",
            "[2024-03-28 21:18:21,057 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 25\n",
            "[2024-03-28 21:18:31,879 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 26\n",
            "[2024-03-28 21:18:38,878 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 27\n",
            "[2024-03-28 21:18:45,618 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 28\n",
            "[2024-03-28 21:18:58,031 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 29\n",
            "[2024-03-28 21:20:00,441 INFO] Step 1300/ 3000; acc: 74.0; ppl:  10.1; xent: 2.3; lr: 0.00245; sents:   30542; bsz: 3862/3419/76; 14991/13273 tok/s;    799 sec;\n",
            "[2024-03-28 21:20:29,566 INFO] Step 1400/ 3000; acc: 76.3; ppl:   9.2; xent: 2.2; lr: 0.00236; sents:   26769; bsz: 3910/3478/67; 53701/47770 tok/s;    828 sec;\n",
            "[2024-03-28 21:20:46,534 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19753)\n",
            "\n",
            "[2024-03-28 21:20:46,535 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 30\n",
            "[2024-03-28 21:20:53,458 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 31\n",
            "[2024-03-28 21:21:03,973 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 32\n",
            "[2024-03-28 21:21:12,570 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 33\n",
            "[2024-03-28 21:22:09,908 INFO] Step 1500/ 3000; acc: 78.9; ppl:   8.2; xent: 2.1; lr: 0.00228; sents:   28563; bsz: 3886/3440/71; 15492/13715 tok/s;    928 sec;\n",
            "[2024-03-28 21:22:40,509 INFO] Step 1600/ 3000; acc: 80.2; ppl:   7.8; xent: 2.1; lr: 0.00221; sents:   28912; bsz: 3874/3430/72; 50641/44832 tok/s;    959 sec;\n",
            "[2024-03-28 21:23:06,439 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19774)\n",
            "\n",
            "[2024-03-28 21:23:06,439 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 34\n",
            "[2024-03-28 21:23:17,079 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 35\n",
            "[2024-03-28 21:23:22,076 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 36\n",
            "[2024-03-28 21:23:34,107 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 37\n",
            "[2024-03-28 21:23:38,858 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 38\n",
            "[2024-03-28 21:24:23,144 INFO] Step 1700/ 3000; acc: 80.8; ppl:   7.6; xent: 2.0; lr: 0.00214; sents:   27615; bsz: 3896/3435/69; 15183/13389 tok/s;   1062 sec;\n",
            "[2024-03-28 21:24:52,942 INFO] Step 1800/ 3000; acc: 83.2; ppl:   6.9; xent: 1.9; lr: 0.00208; sents:   27265; bsz: 3907/3448/68; 52452/46287 tok/s;   1091 sec;\n",
            "[2024-03-28 21:25:24,239 INFO] Step 1900/ 3000; acc: 83.1; ppl:   6.9; xent: 1.9; lr: 0.00203; sents:   28344; bsz: 3872/3438/71; 49485/43947 tok/s;   1123 sec;\n",
            "[2024-03-28 21:25:32,995 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19669)\n",
            "\n",
            "[2024-03-28 21:25:32,995 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 39\n",
            "[2024-03-28 21:25:39,850 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 40\n",
            "[2024-03-28 21:25:44,701 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 41\n",
            "[2024-03-28 21:25:56,563 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 42\n",
            "[2024-03-28 21:26:01,272 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 43\n",
            "[2024-03-28 21:27:02,149 INFO] Step 2000/ 3000; acc: 85.4; ppl:   6.3; xent: 1.8; lr: 0.00198; sents:   31061; bsz: 3863/3427/78; 15784/14000 tok/s;   1221 sec;\n",
            "[2024-03-28 21:27:02,465 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=158)\n",
            "\n",
            "[2024-03-28 21:27:04,674 INFO] valid stats calculation\n",
            "                           took: 2.5226595401763916 s.\n",
            "[2024-03-28 21:27:04,678 INFO] Train perplexity: 18.7034\n",
            "[2024-03-28 21:27:04,678 INFO] Train accuracy: 64.7817\n",
            "[2024-03-28 21:27:04,678 INFO] Sentences processed: 570111\n",
            "[2024-03-28 21:27:04,678 INFO] Average bsz: 3885/3440/71\n",
            "[2024-03-28 21:27:04,678 INFO] Validation perplexity: 8.46579\n",
            "[2024-03-28 21:27:04,678 INFO] Validation accuracy: 80.7258\n",
            "[2024-03-28 21:27:04,678 INFO] Model is improving ppl: 13.7812 --> 8.46579.\n",
            "[2024-03-28 21:27:04,678 INFO] Model is improving acc: 69.2707 --> 80.7258.\n",
            "[2024-03-28 21:27:04,688 INFO] Saving checkpoint models/model.fren_step_2000.pt\n",
            "[2024-03-28 21:27:46,349 INFO] Step 2100/ 3000; acc: 85.4; ppl:   6.3; xent: 1.8; lr: 0.00193; sents:   26399; bsz: 3913/3461/66; 35411/31317 tok/s;   1265 sec;\n",
            "[2024-03-28 21:28:08,924 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19723)\n",
            "\n",
            "[2024-03-28 21:28:08,925 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 44\n",
            "[2024-03-28 21:28:19,725 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 45\n",
            "[2024-03-28 21:28:24,643 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 46\n",
            "[2024-03-28 21:28:30,893 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 47\n",
            "[2024-03-28 21:28:42,935 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 48\n",
            "[2024-03-28 21:29:22,202 INFO] Step 2200/ 3000; acc: 86.2; ppl:   6.1; xent: 1.8; lr: 0.00188; sents:   27438; bsz: 3899/3427/69; 16271/14300 tok/s;   1361 sec;\n",
            "[2024-03-28 21:29:51,754 INFO] Step 2300/ 3000; acc: 88.0; ppl:   5.7; xent: 1.7; lr: 0.00184; sents:   29248; bsz: 3852/3433/73; 52143/46474 tok/s;   1390 sec;\n",
            "[2024-03-28 21:30:22,592 INFO] Step 2400/ 3000; acc: 87.2; ppl:   5.9; xent: 1.8; lr: 0.00180; sents:   28875; bsz: 3898/3448/72; 50568/44721 tok/s;   1421 sec;\n",
            "[2024-03-28 21:30:34,064 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19755)\n",
            "\n",
            "[2024-03-28 21:30:34,065 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 49\n",
            "[2024-03-28 21:30:46,511 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 50\n",
            "[2024-03-28 21:30:52,131 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 51\n",
            "[2024-03-28 21:30:58,148 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 52\n",
            "[2024-03-28 21:32:06,468 INFO] Step 2500/ 3000; acc: 89.4; ppl:   5.4; xent: 1.7; lr: 0.00177; sents:   27589; bsz: 3888/3433/69; 14970/13221 tok/s;   1525 sec;\n",
            "[2024-03-28 21:32:36,595 INFO] Step 2600/ 3000; acc: 88.7; ppl:   5.6; xent: 1.7; lr: 0.00173; sents:   28731; bsz: 3891/3446/72; 51658/45754 tok/s;   1555 sec;\n",
            "[2024-03-28 21:32:51,843 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19716)\n",
            "\n",
            "[2024-03-28 21:32:51,844 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 53\n",
            "[2024-03-28 21:33:02,740 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 54\n",
            "[2024-03-28 21:33:08,227 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 55\n",
            "[2024-03-28 21:33:20,260 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 56\n",
            "[2024-03-28 21:33:25,042 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 57\n",
            "[2024-03-28 21:34:20,625 INFO] Step 2700/ 3000; acc: 89.9; ppl:   5.3; xent: 1.7; lr: 0.00170; sents:   30155; bsz: 3877/3431/75; 14909/13194 tok/s;   1659 sec;\n",
            "[2024-03-28 21:34:50,205 INFO] Step 2800/ 3000; acc: 90.7; ppl:   5.2; xent: 1.6; lr: 0.00167; sents:   28208; bsz: 3884/3450/71; 52519/46658 tok/s;   1689 sec;\n",
            "[2024-03-28 21:35:19,728 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19705)\n",
            "\n",
            "[2024-03-28 21:35:19,728 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 58\n",
            "[2024-03-28 21:35:26,615 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 59\n",
            "[2024-03-28 21:35:37,366 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 60\n",
            "[2024-03-28 21:35:43,609 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 61\n",
            "[2024-03-28 21:35:55,997 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 62\n",
            "[2024-03-28 21:36:29,480 INFO] Step 2900/ 3000; acc: 90.2; ppl:   5.3; xent: 1.7; lr: 0.00164; sents:   26909; bsz: 3885/3419/67; 15654/13776 tok/s;   1788 sec;\n",
            "[2024-03-28 21:36:59,400 INFO] Step 3000/ 3000; acc: 91.8; ppl:   5.0; xent: 1.6; lr: 0.00161; sents:   27377; bsz: 3887/3433/68; 51962/45901 tok/s;   1818 sec;\n",
            "[2024-03-28 21:36:59,612 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=158)\n",
            "\n",
            "[2024-03-28 21:37:01,121 INFO] valid stats calculation\n",
            "                           took: 1.7185585498809814 s.\n",
            "[2024-03-28 21:37:01,124 INFO] Train perplexity: 12.4924\n",
            "[2024-03-28 21:37:01,124 INFO] Train accuracy: 72.771\n",
            "[2024-03-28 21:37:01,124 INFO] Sentences processed: 851040\n",
            "[2024-03-28 21:37:01,124 INFO] Average bsz: 3886/3439/71\n",
            "[2024-03-28 21:37:01,124 INFO] Validation perplexity: 7.45283\n",
            "[2024-03-28 21:37:01,124 INFO] Validation accuracy: 84.7316\n",
            "[2024-03-28 21:37:01,124 INFO] Model is improving ppl: 8.46579 --> 7.45283.\n",
            "[2024-03-28 21:37:01,124 INFO] Model is improving acc: 80.7258 --> 84.7316.\n",
            "[2024-03-28 21:37:01,131 INFO] Saving checkpoint models/model.fren_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!onmt_translate -model models/model.fren_step_3000.pt -src UN.en-fr.fr-filtered.fr.subword.test -output UN.en.translated -gpu 0 -min_length 1\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAQPl7q94-Ck",
        "outputId": "deffb392-9ecd-45ad-ef38-abe852baacf9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-03-28 21:37:27,888 INFO] Loading checkpoint from models/model.fren_step_3000.pt\n",
            "[2024-03-28 21:37:31,912 INFO] Loading data into the model\n",
            "[2024-03-28 21:38:20,014 INFO] PRED SCORE: -0.2346, PRED PPL: 1.26 NB SENTENCES: 2000\n",
            "Time w/o python interpreter load/terminate:  52.13755798339844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade -q sentencepiece\n",
        "\n",
        "# Desubword the translation file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_MOc0hX5Ehc",
        "outputId": "8dbaac70-baf2-4ec1-ba7c-b8e831ecd813"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDone desubwording! Output: UN.en.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 UN.en.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNoFPyL05Ymq",
        "outputId": "c1e512c2-6a33-437b-b1c9-171372bb2fab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁Subcommittee ▁on ▁Prevention\n",
            "▁ 1 3 . ▁Notes ▁with ▁appreciation ▁the ▁recent ▁approach ▁to ▁the ▁establishment ▁of ▁mine - action ▁coordination ▁centres , ▁encourages ▁the ▁establishment ▁of ▁other ▁centres , ▁particularly ▁in ▁emergency ▁situations , ▁and ▁encourages ▁States ▁to ▁support ▁the ▁activities ▁of ▁th ose ▁centres ▁and ▁the ▁trust ▁funds ▁established ▁for ▁the ▁coordination ▁of ▁mine - action ▁activities ;\n",
            "▁Noting ▁the ▁ stated ▁position ▁of ▁the ▁representative ▁of ▁the ▁elected ▁Government ▁of ▁the ▁Pacific ▁regional ▁seminar ▁in ▁ 2 0 0 4 ▁that ▁the ▁people ▁of ▁the ▁Territory ▁ did ▁not ▁fully ▁understand ▁all ▁the ▁possibilities ▁or ▁the ▁significance ▁of ▁the ▁v arious ▁ self - determination ▁options ▁that ▁might ▁be ▁available ▁to ▁them , ▁and ▁noting ▁also ▁the ▁review ▁of ▁the ▁Constitution ,\n",
            "▁Add ▁indicators ▁of ▁achievement ▁as ▁( c ) ▁of ▁paragraph ▁ 1 7 . 3 1 7 ▁of ▁the ▁medium - term ▁plan ▁for ▁the ▁period ▁ 2 0 0 2 - 2 0 0 5 , ▁as ▁revised , ▁as ▁indicators ▁of ▁achievement ▁( a ) ▁( iv ).\n",
            "▁RESOLUTION ▁ 6 2 / 8 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 UN.en-fr.fr-filtered.fr.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5awr31qO5b9B",
        "outputId": "de7489a7-cff0-4ffa-d346-4a870f52ff9a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁Sous - Comité ▁de ▁la ▁prévention\n",
            "▁ 1 3 . ▁Note ▁avec ▁satisfaction ▁les ▁démarche s ▁suivies ▁récemment ▁en ▁ce ▁qui ▁concerne ▁la ▁création ▁de ▁centres ▁de ▁coordination ▁de ▁l ' action ▁antimines , ▁encourage ▁la ▁création ▁d ' autres ▁centres , ▁en ▁particulier ▁d ans ▁les ▁situations ▁d ' urgence , ▁et ▁encourage ▁également ▁les ▁États ▁à ▁appuyer ▁les ▁activités ▁de ▁ces ▁centres ▁et ▁des ▁fonds ▁d ' affectation ▁spéciale ▁créés ▁pour ▁coordonner ▁l ' assistance ▁à ▁l ' action ▁antimines ▁sous ▁les ▁au s pices ▁du ▁Service ▁de ▁l ' action ▁antimines ;\n",
            "▁Notant ▁la ▁position ▁adoptée ▁par ▁le ▁représentant ▁du ▁gouvernement ▁élu ▁au ▁séminaire ▁régional ▁pour ▁le ▁Pacifique ▁de ▁ 2 0 0 4 , ▁s elon ▁laquelle ▁les ▁habitants ▁du ▁territoire ▁ne ▁comprennent ▁pas ▁pleinement ▁tout ▁l ' intérêt ▁ou ▁la ▁significati on ▁des ▁diverses ▁option s ▁en ▁matière ▁d ' a utodétermination ▁dont ▁ ils ▁pourraient ▁se ▁prévaloir , ▁et ▁notant ▁également ▁que ▁la ▁révision ▁de ▁la ▁Constitution ▁a ▁été ▁reportée ,\n",
            "▁Ajouter , ▁en ▁ tant ▁qu ' indicateurs ▁de ▁succès ▁iii , ▁iv ▁et ▁v , ▁les ▁indicateurs ▁de ▁succès ▁ cités ▁au ▁paragraphe ▁ 1 7 . 1 3 ▁du ▁plan ▁à ▁moyen ▁terme ▁révisé ▁pour ▁la ▁période ▁ 2 0 0 2 - 2 0 0 5 .\n",
            "▁RÉSOLUTION ▁ 6 2 / 8 1\n"
          ]
        }
      ]
    }
  ]
}