{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJBOt3NyNOq"
   },
   "source": [
    "# MIND: A Large-scale Datset for News Recommendation\n",
    "\n",
    "## COMP9727 Project Implementation and Evaluation\n",
    "\n",
    "Linbo Zhang z5352294, Jinghan Wang z5286124, Junyu Li z5467278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX7IUFPC35eZ"
   },
   "source": [
    "\n",
    "## Problem\n",
    "\n",
    "The project aims to implement and evaluate a news recommendation system based on the MIND dataset. The MIND dataset is a large-scale dataset for news recommendation, which was released by Microsoft. The dataset contains news articles and user behaviors (clicks). The goal of the project is to build a news recommendation system that can recommend news articles to users based on their historical behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEs7NR9ryNOr"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2J1VTE0yNOr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcLCfEHtyNOs"
   },
   "outputs": [],
   "source": [
    "df_behaviors = pd.read_csv('MINDlarge_train/behaviors.tsv', sep='\\t', header=None)\n",
    "columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "df_behaviors.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-bDqvYCyNOs",
    "outputId": "6d097110-f7c0-4257-f1de-52c5f64e6015"
   },
   "outputs": [],
   "source": [
    "print(df_behaviors.shape)\n",
    "df_behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIWZMtK9yNOs",
    "outputId": "6459cbeb-145b-4067-ded6-b8a422202558"
   },
   "outputs": [],
   "source": [
    "# the date format is in MM/DD/YYYY HH:MM:SS, so need to convert to a datetime object\n",
    "df_behaviors['Time'] = pd.to_datetime(df_behaviors['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "df_behaviors['Time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtz8aFuAyNOs"
   },
   "source": [
    "Total 5 columns. We can ignore the first column `Impression ID`.\n",
    "\n",
    "- `User ID`: the unique identifier of the user.\n",
    "- `Time`: The timestamp of the impression\n",
    "- `History`: The news that the user has viewed before this impression. Each news is separated by a space. And it is a string at the moment.\n",
    "- `Impressions`: The news that the user has viewed in this impression. Each news contains `News_ID-<click status>`, so 1 means the user clicks on the news, and 0 means the user does not click on the news. Each news is separated by a space.\n",
    "\n",
    "Check how many NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkRbqXmmyNOs",
    "outputId": "f6723bfe-d532-4ac1-b68f-c5e32497c54b"
   },
   "outputs": [],
   "source": [
    "# check NA\n",
    "df_behaviors.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc2vEu9OyNOs"
   },
   "source": [
    "Among all the 2232748 rows, there are 46065 with empty history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvJvoFIvyNOt",
    "outputId": "979418ae-1b3a-416c-a1df-a4d7cd158af8"
   },
   "outputs": [],
   "source": [
    "# check how many unique user Id\n",
    "print(df_behaviors['User_ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rocVLMsyNOt"
   },
   "source": [
    "There are total 2,232,748 records, so more than 2.2 million records. And there are total 711,222 unique users.\n",
    "\n",
    "So many users have multiple records.\n",
    "\n",
    "So when we look back to the 46065 records with empty history, we may replace the NA values with the most closest history of the same user.\n",
    "\n",
    "According to the dataset description, they sampled 1 million users who had at least 5 news clicks during 6 weeks from October 12 to November 22, in year 2019.\n",
    "\n",
    "The click behaviors in the first 4 weeks are used to construct the news click history for user modeling.\n",
    "\n",
    "The samples in the last day of the fifth week is the validation set.\n",
    "\n",
    "So we can check the date range using the `Time` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xmvn_j9eyNOt",
    "outputId": "24a2766d-4182-465c-8882-1d34121281f1"
   },
   "outputs": [],
   "source": [
    "# check date range using the Time column\n",
    "print(df_behaviors['Time'].min())\n",
    "print(df_behaviors['Time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vHpaARFyNOt",
    "outputId": "58289f6f-fde7-41a3-eef0-88b327250532"
   },
   "outputs": [],
   "source": [
    "# here we also read the val hebaviors and test behaviors csv to compare the time range\n",
    "df_behaviors_val = pd.read_csv('MINDlarge_dev/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors_val.columns = columns\n",
    "\n",
    "df_behaviors_test = pd.read_csv('MINDlarge_test/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors_test.columns = columns\n",
    "\n",
    "# convert the time fo rthe val and test set\n",
    "df_behaviors_val['Time'] = pd.to_datetime(df_behaviors_val['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "df_behaviors_test['Time'] = pd.to_datetime(df_behaviors_test['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "print(\"Validation set\")\n",
    "print(df_behaviors_val['Time'].min())\n",
    "print(df_behaviors_val['Time'].max())\n",
    "\n",
    "print(\"Test set\")\n",
    "print(df_behaviors_test['Time'].min())\n",
    "print(df_behaviors_test['Time'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCdpriuVyNOt"
   },
   "source": [
    "Let's summarize the date range for the 3 sets\n",
    "\n",
    "| Set | Min Date | Max Date |\n",
    "| --- | --- | --- |\n",
    "| Train | 2019-11-09 00:00:00 | 2019-11-14 23:59:59 |\n",
    "| Val | 2019-11-15 00:00:00 | 2019-11-15 23:59:43 |\n",
    "| Test | 2019-11-16 00:00:00 | 2019-11-22 23:59:58 |\n",
    "\n",
    "Hence the training data happens over 6 days. And the data on the 7th day is used as validation set.\n",
    "\n",
    "Then the next week's data is used as the test set.\n",
    "\n",
    "Notice that this is a time series data.\n",
    "\n",
    "Now we look further into the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pzrXBwfyNOt",
    "outputId": "80d68660-42c4-41c8-b8f5-754437f3950b"
   },
   "outputs": [],
   "source": [
    "# print the first row history, and first row impression\n",
    "print(df_behaviors['History'][0])\n",
    "print()\n",
    "print(df_behaviors['Impressions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe7ah7vtyNOt"
   },
   "source": [
    "The history is a string of news IDs.\n",
    "\n",
    "And the `Impressions` list is also a list of news IDs. Each news in the impression column also comes with the click status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsjOlGVSyNOt",
    "outputId": "c9a1b853-50f2-4a27-a8c5-cbb0aeee6782"
   },
   "outputs": [],
   "source": [
    "# split the history, check how many items in the history, (the average number)\n",
    "# and split the impressions, check how many items in the impressions, (the average number)\n",
    "df_behaviors['History_Length'] = df_behaviors['History'].apply(lambda x: len(x.split()) if type(x) == str else np.nan)\n",
    "df_behaviors['Impressions_Length'] = df_behaviors['Impressions'].apply(lambda x: len(x.split()) if type(x) == str else np.nan)\n",
    "\n",
    "print(df_behaviors['History_Length'].mean())\n",
    "print(df_behaviors['Impressions_Length'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmd3i98oyNOt"
   },
   "source": [
    "On average, the user is exposed to 33.67 news in the history, then the impression contains on average 37.40 news.\n",
    "\n",
    "This is quite the same experience when we open the news website or news app.\n",
    "\n",
    "Now we look at the news data. It is a separate tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqSvkVQPyNOt",
    "outputId": "04c90b32-49ed-4f54-ea60-89e06d7e5064"
   },
   "outputs": [],
   "source": [
    "df_news = pd.read_csv('MINDlarge_train/news.tsv', sep='\\t', header=None)\n",
    "columns = [\"News_ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title_Entities\", \"Abstract_Entities\"]\n",
    "df_news.columns = columns\n",
    "\n",
    "print(df_news.shape)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5yOMjgByNOt"
   },
   "source": [
    "There are total 101527 news (over a million news).\n",
    "\n",
    "There are 8 columns in the news data.\n",
    "\n",
    "- News_ID: the unique identifier for each news\n",
    "- Category\n",
    "- Subcategory\n",
    "- Title\n",
    "- Abstract\n",
    "- URL: the news url. but since the dataset is from 2019, most urls cannot be open anymore. At that time, many models are trained using the news content. But we will not do that since these contents are no longer available.\n",
    "- Title Entities: entities contained in the title, it includes label, type, wikidataId, confidence, occurrenceOffsets, SurfaceForms. These are gathered by the dataset provider.\n",
    "- Abstract Entities: similar to the title entities.\n",
    "\n",
    "Now we look at how many cells are NA in the news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NAs1J8JyNOu",
    "outputId": "611674f5-dc04-4d09-9fb8-0792472e49c1"
   },
   "outputs": [],
   "source": [
    "# check NA\n",
    "df_news.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiD8rY-kyNOu"
   },
   "source": [
    "Among the 1 million news, 5415 does not have an abstract. And only a few is missing the title and abstract entities.\n",
    "\n",
    "We check if the news ID is all unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qk3F9CNyNOu",
    "outputId": "3a546818-3a36-4f23-b33d-41ecfeddb395"
   },
   "outputs": [],
   "source": [
    "print(df_news['Category'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GsFLSDJyNOu"
   },
   "source": [
    "So there are 18 duplicated news IDs. That will be removed later.\n",
    "\n",
    "Now we do some analysis on the length of the title and abstract, as the user only sees the title and abstract, before deciding to click on the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yrV1Je0yNOu"
   },
   "outputs": [],
   "source": [
    "# for the Title, count the number of words\n",
    "# skip NaN values\n",
    "df_news['Title_Length'] = df_news['Title'].apply(lambda x: len(x.split()) if type(x) == str else 0)\n",
    "\n",
    "# for the abstract length, skip NaN values\n",
    "df_news['Abstract_Length'] = df_news['Abstract'].apply(lambda x: len(x.split()) if type(x) == str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDc7mtouyNOu",
    "outputId": "3eb88b6d-f41f-4a7d-e905-7a33e1faeec0"
   },
   "outputs": [],
   "source": [
    "title_length_count = df_news['Title_Length'].value_counts().sort_index()\n",
    "total_titles = title_length_count.sum()\n",
    "title_length_percentage = title_length_count / total_titles * 100\n",
    "\n",
    "# print the average title length and mode\n",
    "print(\"mean value: \", df_news['Title_Length'].mean())\n",
    "print(\"mode value: \", df_news['Title_Length'].mode()[0])\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "title_length_percentage.plot(kind='bar')\n",
    "plt.xlabel('Title Length (Number of Words)')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Frequency Percentage Plot of Title Lengths on Training Set')\n",
    "\n",
    "# make the xticks 45\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXbFHbXgyNOu"
   },
   "source": [
    "The mean length is 10.7 words. And the mode is 10 words. So that a good news title should be around 10 words, i.e. keep the title short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEij5s3syNOu",
    "outputId": "c7005754-c0be-4cee-a440-6196441463ae"
   },
   "outputs": [],
   "source": [
    "# same for the abstract\n",
    "abstract_length_count = df_news['Abstract_Length'].value_counts().sort_index()\n",
    "total_abstracts = abstract_length_count.sum()\n",
    "abstract_length_percentage = abstract_length_count / total_abstracts * 100\n",
    "\n",
    "# print mean and mode\n",
    "print(\"mean value: \", df_news['Abstract_Length'].mean())\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "abstract_length_percentage.plot(kind='bar')\n",
    "plt.xlabel('Abstract Length (Number of Words)')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Frequency Plot of Abstract Lengths on Training Set')\n",
    "\n",
    "# Step 4: Reduce the number of xticks\n",
    "step = 20  # Choose an appropriate step size\n",
    "plt.xticks(ticks=range(0, abstract_length_percentage.index.max() + 1, step))\n",
    "\n",
    "# make the xticks upright\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# set the xlim between 0 to 200\n",
    "plt.xlim(0, 140)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY-HwE2tyNOu"
   },
   "source": [
    "The situation is different for the abstract.\n",
    "\n",
    "The abstract is longer, so the mean number of words is 36.24 words.\n",
    "\n",
    "And we see there are two modes, one is around 20, and the other one is around 70. That means the abstract can be short or long.\n",
    "\n",
    "Now we check one interestring concept in the news data, the survival time. It is defined as the time interval between the first appearance of the news and the last appearance of the news. The value calculated from the dataset could be inaccurate since we only have the impression time. But it is a good estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BLdWcSKyNOu"
   },
   "outputs": [],
   "source": [
    "# check the survival time for each news\n",
    "# defined using the time interval between its first and last appearance time in the dataset\n",
    "\n",
    "# take the time and impressions coumn\n",
    "df_survival = df_behaviors[['Time', 'Impressions']]\n",
    "\n",
    "news_with_time = []\n",
    "\n",
    "# for each row in the df_survival, split the impressions. and remove -0 or -1 at the end\n",
    "# then insert a tuple of the (news id, time) into the news_with_time list\n",
    "for idx, row in df_survival.iterrows():\n",
    "    dt = row['Time']\n",
    "    for impression in row['Impressions'].split():\n",
    "        news_id, click = impression.split('-')\n",
    "        news_with_time.append((news_id, dt))\n",
    "\n",
    "# convert the news_with_time list into a dataframe\n",
    "df_survival = pd.DataFrame(news_with_time, columns=['News_ID', 'Time'])\n",
    "\n",
    "# group by the News_ID and get the earliest and latest time\n",
    "df_survival_grouped = df_survival.groupby('News_ID')['Time'].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "# get the difference between the two\n",
    "df_survival_grouped['Survival_Time'] = df_survival_grouped[\"max\"] - df_survival_grouped[\"min\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU2TpqIgyNOu",
    "outputId": "904b40f5-7015-4ed3-8b29-785cbb55022c"
   },
   "outputs": [],
   "source": [
    "# plot a histogram of the survival time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# convert to days, but in float\n",
    "plt.hist(df_survival_grouped['Survival_Time'].dt.total_seconds() / 3600 / 24, bins=50)\n",
    "plt.xlabel('Survival Time (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Survival Time of News Articles in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1MbLEkXyNOu"
   },
   "source": [
    "So most news have a survival time less than 1 day.\n",
    "\n",
    "That means most news are short-lived.\n",
    "\n",
    "The longest survival is 6 days which is inaccurate, as the dataset only contains 6 days of data.\n",
    "\n",
    "Now we plot to see the distribution of category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg600e-KyNOu",
    "outputId": "71c6f9bd-2e8e-4470-9b21-cbeeb01b1378"
   },
   "outputs": [],
   "source": [
    "# use the df_news[\"Category\"] column to plot a pie chart\n",
    "category_count = df_news['Category'].value_counts()\n",
    "total_news = category_count.sum()\n",
    "category_percentage = category_count / total_news * 100\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "patches, texts = plt.pie(category_count, startangle=90, counterclock=False)\n",
    "\n",
    "# do the labels separately\n",
    "labels_with_percentage = [f'{label} ({percentage:.2f}%)' for label, percentage in zip(category_count.index, category_percentage)]\n",
    "plt.legend(patches, labels_with_percentage, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.title('Pie Chart of News Categories on Training Set')\n",
    "plt.ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrScRXidyNOu"
   },
   "source": [
    "So the top 2 popular categories are sports and news.\n",
    "\n",
    "At last, we have a look at the validation set and test set. And check the NA existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6BgfuwFyNOu"
   },
   "outputs": [],
   "source": [
    "behavior_columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "news_columns = [\"News_ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title_Entities\", \"Abstract_Entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPt36gICyNOv",
    "outputId": "d546288c-99a0-4c7d-b436-c4cc45010841"
   },
   "outputs": [],
   "source": [
    "# check the shape of the validation set and the test set\n",
    "df_behaviors_val = pd.read_csv('MINDlarge_dev/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors_val.columns = behavior_columns\n",
    "\n",
    "print(\"Validation Set behaviors.tsv\")\n",
    "print(df_behaviors_val.shape)\n",
    "print(df_behaviors_val.isna().sum())\n",
    "print()\n",
    "\n",
    "df_news_val = pd.read_csv('MINDlarge_dev/news.tsv', sep='\\t', header=None)\n",
    "df_news_val.columns = news_columns\n",
    "\n",
    "print(\"Validation Set news.tsv\")\n",
    "print(df_news_val.shape)\n",
    "print(df_news_val.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bl6ACW3GyNOv",
    "outputId": "e8ccc3ab-be9e-4c8d-b39e-d4692f6326fb"
   },
   "outputs": [],
   "source": [
    "# for the test set\n",
    "df_behaviors_test = pd.read_csv('MINDlarge_test/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors_test.columns = behavior_columns\n",
    "\n",
    "print(\"Test Set behaviors.tsv\")\n",
    "print(df_behaviors_test.shape)\n",
    "print(df_behaviors_test.isna().sum())\n",
    "print()\n",
    "\n",
    "df_news_test = pd.read_csv('MINDlarge_test/news.tsv', sep='\\t', header=None)\n",
    "df_news_test.columns = news_columns\n",
    "\n",
    "print(\"Test Set news.tsv\")\n",
    "print(df_news_test.shape)\n",
    "print(df_news_test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axik1QwJyNOv"
   },
   "source": [
    "The NA existence is similar. Some history column in the behavior.csv is empty. And some abstract column in the news.tsv is empty.\n",
    "\n",
    "And a small number of URL, title_entities, abstract_entities are empty in the news.tsv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds7sZALpyNOv"
   },
   "source": [
    "## Methods: Use the MINDsmall to train and test the model\n",
    "\n",
    "The original dataset is too large. And our computer resources are very limited.\n",
    "\n",
    "So we will use the MINDsmall dataset.\n",
    "\n",
    "Unfortunately, we only have the train version of the small dataset. The small validation set cannot be downloaded.\n",
    "\n",
    "So we will use the user_id inside the train dataset to extract the relevant data from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DlR0wTVyNOv",
    "outputId": "c316130a-4495-4246-8509-4cc70a36f221"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_behaviors_small = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors_large = pd.read_csv('MINDlarge_train/behaviors.tsv', sep='\\t', header=None)\n",
    "\n",
    "df_behaviors_small.columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "df_behaviors_large.columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "\n",
    "# check the shape of the two dataframes\n",
    "print(df_behaviors_small.shape)\n",
    "print(df_behaviors_large.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pj2XBlyyNOv",
    "outputId": "9d1c7534-fd0c-4832-bb15-9616d63a26dc"
   },
   "outputs": [],
   "source": [
    "# check the number of unique people inside the two dataframes\n",
    "print(df_behaviors_small['User_ID'].nunique())\n",
    "print(df_behaviors_large['User_ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt1RLqEyyNOw"
   },
   "source": [
    "The small training set is a subset of the large set. It contains only 50,000 users.\n",
    "\n",
    "156965/2232748 = 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up_54n4ByNOw",
    "outputId": "821e085e-247f-4e62-bf9a-a760dca79eae"
   },
   "outputs": [],
   "source": [
    "# open the large dev and test set behaviors.tsv file\n",
    "# and check the number of unique people inside the two dataframes\n",
    "df_behaviors_val = pd.read_csv('MINDlarge_dev/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors_test = pd.read_csv('MINDlarge_test/behaviors.tsv', sep='\\t', header=None)\n",
    "\n",
    "df_behaviors_val.columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "df_behaviors_test.columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "\n",
    "print(df_behaviors_val['User_ID'].nunique())\n",
    "print(df_behaviors_test['User_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nooNClzHyNOw",
    "outputId": "382a2d97-ebf5-4aa2-a045-bca348a6b381"
   },
   "outputs": [],
   "source": [
    "# keep only the same 50000 users in the large dataset\n",
    "keep_user_ids = df_behaviors_small['User_ID'].unique()\n",
    "df_behaviors_val = df_behaviors_val[df_behaviors_val['User_ID'].isin(keep_user_ids)]\n",
    "df_behaviors_test = df_behaviors_test[df_behaviors_test['User_ID'].isin(keep_user_ids)]\n",
    "\n",
    "# check the shape of the two dataframes\n",
    "print(df_behaviors_val.shape)\n",
    "print(df_behaviors_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oofuTLfyNOw"
   },
   "source": [
    "If we keep the same 50,000 users in both validation and test set,\n",
    "\n",
    "for validation, remain 22880 out of 255990 which is 8.9%\n",
    "\n",
    "for testing, remain 140593 out of 702005, which is 20%.\n",
    "\n",
    "From the percentage, the selected 50000 users play an important role in the test set.\n",
    "\n",
    "Now we save the filtered dataframe into the folder. And we manually move all other files into the folder as well.\n",
    "\n",
    "So we have `MINDsmall_train`, `MINDsmall_dev`, `MINDsmall_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03ro-TUxyNOw"
   },
   "outputs": [],
   "source": [
    "# output the two files into MINDsmall_dev and MINDsmall_test\n",
    "import os\n",
    "if not os.path.exists('MINDsmall_dev'):\n",
    "    os.makedirs('MINDsmall_dev')\n",
    "\n",
    "if not os.path.exists('MINDsmall_test'):\n",
    "    os.makedirs('MINDsmall_test')\n",
    "\n",
    "df_behaviors_val.to_csv('MINDsmall_dev/behaviors.tsv', sep='\\t', index=False, header=False)\n",
    "df_behaviors_test.to_csv('MINDsmall_test/behaviors.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# and also copy all the remaining files into the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZWLqCjayNOw"
   },
   "source": [
    "## Method: Prepare the two datasets\n",
    "\n",
    "1. Load the news.tsv file, replace NA with empty strings, and remove the duplicated news IDs.\n",
    "2. Concat the title, abstract, category, subcategory into a single string. Lowercase the new string column, and apply TF-IDF to convert the text into vectors.\n",
    "3. Create a dictionary of the {news_id: vector} for the news data\n",
    "4. Load the behaviors.tsv file, and replace NA with empty strings.\n",
    "5. Split the impressions column via space, and explode the column into multiple rows.\n",
    "6. Further split the impressions into news_id and click status. Convert the click status into integers.\n",
    "7. For the history column, based ont eh dictionary in step 3, convert the history column into a vector, using the mean of the vectors of the news.\n",
    "8. Convert the news_id into vector as well.\n",
    "9. Now we have the user_id, history vector, news vector, and the click status.\n",
    "10. Use label encoder to fit and transform on the user_id. And another label encoder to fit and transform on the news_id.\n",
    "\n",
    "The following codes go through the step. And finally we will provide a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoToslANyNOw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40fUqBZLyNOw"
   },
   "outputs": [],
   "source": [
    "TF_VECTOR_LENGTH = 5000\n",
    "\n",
    "behavior_columns = [\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\"]\n",
    "news_columns = [\"News_ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title_Entities\", \"Abstract_Entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWjv8iLVyNOw",
    "outputId": "8eeaf5e6-d295-43cc-d617-c5551ab67bd9"
   },
   "outputs": [],
   "source": [
    "# read the news.tsv first, replace NA with empty strings\n",
    "df_news = pd.read_csv('MINDsmall_train/news.tsv', sep='\\t', header=None)\n",
    "df_news.columns = news_columns\n",
    "df_news.fillna(\"\", inplace=True)\n",
    "\n",
    "# concat the title, abstract, category, subcategory into a single string\n",
    "df_news['Text'] = df_news['Title'] + ' ' + df_news['Abstract'] + ' ' + df_news['Category'] + ' ' + df_news['Subcategory']\n",
    "df_news['Text'] = df_news['Text'].str.lower()\n",
    "\n",
    "# initialize the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=TF_VECTOR_LENGTH)\n",
    "\n",
    "# fit the TfidfVectorizer on the text, and save the result into a new column\n",
    "df_news['tfidf'] = list(tfidf.fit_transform(df_news['Text']).toarray())\n",
    "\n",
    "# check the shape\n",
    "print(df_news.shape)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2oDpdjjyNOw",
    "outputId": "be5f8f0f-5cd4-4512-9974-18d60d6be060"
   },
   "outputs": [],
   "source": [
    "# obtain a tfidf vector\n",
    "v = df_news['tfidf'][0]\n",
    "print(np.shape(v))\n",
    "print(v)\n",
    "\n",
    "# check unique values from the v\n",
    "print(np.unique(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgaascICyNOw"
   },
   "source": [
    "So each vector is fixed at some number of features. And it is saveed into the tfidf column. So for the news tsv, we only need to keep the News_ID and tfidf columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhfrcWMCyNOw",
    "outputId": "bd0810fe-8918-4631-ef7c-270403a55991"
   },
   "outputs": [],
   "source": [
    "df_news = df_news[['News_ID', 'tfidf']]\n",
    "print(df_news.shape)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmBrSiCfyNOw"
   },
   "outputs": [],
   "source": [
    "# save the two columns into a dictionary\n",
    "news_dict = dict(zip(df_news['News_ID'], df_news['tfidf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ela54fsuyNOw",
    "outputId": "0d5fba53-5cc0-43fa-8688-38f896dd54cc"
   },
   "outputs": [],
   "source": [
    "# read the behaviors.tsv file\n",
    "# drop rows with NA values\n",
    "# convert time\n",
    "df_behaviors = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None)\n",
    "df_behaviors.columns = behavior_columns\n",
    "\n",
    "df_behaviors.dropna(inplace=True)\n",
    "\n",
    "df_behaviors['Time'] = pd.to_datetime(df_behaviors['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "print(df_behaviors.shape)\n",
    "df_behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAdqZrQmyNOw",
    "outputId": "7652d777-3470-4a3e-cc04-1c4d8375236c"
   },
   "outputs": [],
   "source": [
    "# split the Impressions column via space, and explode it\n",
    "df_behaviors[\"Impressions\"] = df_behaviors[\"Impressions\"].apply(lambda x: x.split()).explode().reset_index(drop=True)\n",
    "\n",
    "# now further split the Impressions into the News_ID and Click\n",
    "df_behaviors[['News_ID', 'Click']] = df_behaviors['Impressions'].str.split('-', expand=True)\n",
    "\n",
    "# convert the click into int\n",
    "df_behaviors['Click'] = df_behaviors['Click'].astype(int)\n",
    "\n",
    "# drop the Impressions column\n",
    "df_behaviors.drop(columns=[\"Impressions\", \"Impression_ID\", \"Time\"], inplace=True)\n",
    "\n",
    "print(df_behaviors.shape)\n",
    "df_behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1c7Ca58yNOx",
    "outputId": "fdb51feb-4c38-4f37-de9f-235689d0f8b8"
   },
   "outputs": [],
   "source": [
    "# so for each history cell, need to split them and get each tfidf vector, then take the average\n",
    "def get_avg_tfidf_history(history):\n",
    "    news_ids = history.split()\n",
    "    tfidf_vectors = [news_dict[news_id] for news_id in news_ids if news_id in news_dict]\n",
    "    if len(tfidf_vectors) > 0:\n",
    "        return np.mean(tfidf_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(5000)\n",
    "\n",
    "# apply the function to the History column\n",
    "df_behaviors[\"history_tfidf\"] = df_behaviors[\"History\"].apply(get_avg_tfidf_history)\n",
    "\n",
    "# for the News_ID also need to get the tfidf\n",
    "df_behaviors[\"news_tfidf\"] = df_behaviors[\"News_ID\"].apply(lambda x: news_dict[x] if x in news_dict else np.zeros(5000))\n",
    "\n",
    "# drop the History\n",
    "df_behaviors.drop(columns=[\"History\"], inplace=True)\n",
    "\n",
    "print(df_behaviors.shape)\n",
    "df_behaviors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKTQC6JsyNOx"
   },
   "source": [
    "In the LibFM model, we need to concat the history_tfidf and news_tfidf into a single vector.\n",
    "\n",
    "For the deep learning model, we don't need to concat them.\n",
    "\n",
    "Here we provide a function to do the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3b1eyIVyNOx"
   },
   "outputs": [],
   "source": [
    "def process_tsv_files(behaviors_tsv_path, news_tsv_path, tf_length=TF_VECTOR_LENGTH):\n",
    "    df_news = pd.read_csv(news_tsv_path, sep='\\t', header=None)\n",
    "    df_news.columns = news_columns\n",
    "    df_news.fillna(\"\", inplace=True)\n",
    "\n",
    "    df_news['Text'] = df_news['Title'] + ' ' + df_news['Abstract'] + ' ' + df_news['Category'] + ' ' + df_news['Subcategory']\n",
    "    df_news['Text'] = df_news['Text'].str.lower()\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=tf_length)\n",
    "    df_news['tfidf'] = list(vectorizer.fit_transform(df_news['Text']).toarray())\n",
    "\n",
    "    # dictionary\n",
    "    news_dict = dict(zip(df_news['News_ID'], df_news['tfidf']))\n",
    "\n",
    "    df_behaviors = pd.read_csv(behaviors_tsv_path, sep='\\t', header=None)\n",
    "    df_behaviors.columns = behavior_columns\n",
    "    df_behaviors.dropna(inplace=True)\n",
    "\n",
    "    df_behaviors['Time'] = pd.to_datetime(df_behaviors['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "    df_behaviors[\"Impressions\"] = df_behaviors[\"Impressions\"].apply(lambda x: x.split()).explode().reset_index(drop=True)\n",
    "    df_behaviors[['News_ID', 'Click']] = df_behaviors['Impressions'].str.split('-', expand=True)\n",
    "    df_behaviors['Click'] = df_behaviors['Click'].astype(int)\n",
    "\n",
    "    def get_avg_tfidf_history(history):\n",
    "        news_ids = history.split()\n",
    "        tfidf_vectors = [news_dict[news_id] for news_id in news_ids if news_id in news_dict]\n",
    "        if len(tfidf_vectors) > 0:\n",
    "            return np.mean(tfidf_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(tf_length)\n",
    "\n",
    "    df_behaviors[\"history_tfidf\"] = df_behaviors[\"History\"].apply(get_avg_tfidf_history)\n",
    "    df_behaviors[\"news_tfidf\"] = df_behaviors[\"News_ID\"].apply(lambda x: news_dict[x] if x in news_dict else np.zeros(tf_length))\n",
    "\n",
    "    # only keep the necessary columns\n",
    "    df_behaviors = df_behaviors[['User_ID', 'history_tfidf', 'news_tfidf', 'Click', 'News_ID']]\n",
    "\n",
    "    # label encoder\n",
    "    user_encoder = LabelEncoder()\n",
    "    df_behaviors['User_ID'] = user_encoder.fit_transform(df_behaviors['User_ID'])\n",
    "\n",
    "    news_encoder = LabelEncoder()\n",
    "    news_encoder.fit(df_news['News_ID'])\n",
    "    df_behaviors['News_ID'] = news_encoder.transform(df_behaviors['News_ID'])\n",
    "\n",
    "    # return the two dataframes, and the vectorizer, and the two encoder\n",
    "    return df_behaviors, df_news, vectorizer, user_encoder, news_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYvYo-BoyNOx"
   },
   "source": [
    "## Model: LibFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlO3qrKiyNOx"
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60oUIrpcyNOx"
   },
   "source": [
    "* Model Type: Hybrid Recommender System combining Collaborative Filtering and Content-Based Filtering.\n",
    "* Data Input: Uses an interaction matrix for user-item interactions and feature matrices for item and user attributes.\n",
    "* Training Optimization: Utilizes WARP, BPR, and log loss functions for improved ranking and personalization.\n",
    "* Embeddings: Generates dense vector representations for users and items to calculate scores.\n",
    "* Evaluation Metric: Assesses performance using the AUC score to measure the model's ability to distinguish relevant items.\n",
    "\n",
    "The LightFM model is a hybrid recommender system that integrates collaborative filtering and content-based filtering. It uses an interaction matrix for user-item interactions and feature matrices for additional item and user attributes. Training the model involves optimizing advanced loss functions like WARP, BPR, and log loss, which improve ranking and personalization. The model creates dense vector embeddings for users and items, using the dot product of these embeddings to score and rank items for recommendations. Performance is evaluated using metrics like the AUC score, which measures the ability to distinguish between relevant and non-relevant items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocu2JUP1D-rX"
   },
   "source": [
    "This is a relatively early model, similar to machine learning in nature. According to the paper, this is a classical recommendation method based on a factorization machine.\n",
    "\n",
    "In this model, we input the user ID, news ID, and the TF-IDF vector of the news. This vector consists of two parts: the first part represents the historical news vector, and the second part represents the vector of the news being recommended to the user. The user then decides whether to click on the news, which we refer to as a “click.”\n",
    "\n",
    "Our prediction is focused on whether the user will click on the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cX9raqL1ECQy",
    "outputId": "27471b58-4ede-41f7-e778-030808558b2a"
   },
   "outputs": [],
   "source": [
    "# concat the two vectors into a single one\n",
    "df_behaviors_train, df_news_train, vectorizer, user_encoder, news_encoder = process_tsv_files(\"MINDsmall_train/behaviors.tsv\", \"MINDsmall_train/news.tsv\")\n",
    "df_behaviors_train[\"combined_tfidf\"] = df_behaviors_train.apply(lambda x: np.concatenate([x[\"history_tfidf\"], x[\"news_tfidf\"]]), axis=1)\n",
    "\n",
    "# initialize the dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=df_behaviors_train[\"User_ID\"].unique(),\n",
    "    items=df_behaviors_train[\"News_ID\"].unique(),\n",
    "    item_features=[f\"{i}\" for i in range(len(df_behaviors_train['combined_tfidf'][0]))]\n",
    ")\n",
    "\n",
    "# check the number of users and number of items\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "print(num_users, num_items)\n",
    "\n",
    "# check the item features\n",
    "num_item_features = dataset.item_features_shape()\n",
    "print(num_item_features)\n",
    "\n",
    "# build the interactions\n",
    "# input the user_id, item_id and the weight,\n",
    "# return a tuple of the interactions and the weights\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    (row['User_ID'], row['News_ID'], row['Click']) for index, row in df_behaviors_train.iterrows()\n",
    ")\n",
    "\n",
    "item_features = dataset.build_item_features(\n",
    "    (\n",
    "        row[\"News_ID\"],\n",
    "        {f\"{i}\": v for i, v in enumerate(row[\"combined_tfidf\"]) if v != 0}\n",
    "    ) for index, row in df_behaviors_train.iterrows()\n",
    ")\n",
    "\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(interactions, item_features=item_features, epochs=10, num_threads=1, verbose=True)\n",
    "\n",
    "from lightfm.evaluation import auc_score\n",
    "train_auc = auc_score(model, interactions, item_features=item_features, num_threads=16).mean()\n",
    "print(f'Train AUC: {train_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byWhWGplEI5D"
   },
   "outputs": [],
   "source": [
    "train_auc = auc_score(model, interactions, item_features=item_features, num_threads=1).mean()\n",
    "print(f'Train AUC: {train_auc}')\n",
    "\n",
    "# Predict scores for all user-item pairs\n",
    "def predict_scores(model, interactions, item_features):\n",
    "    num_users, num_items = interactions.shape\n",
    "    scores = np.empty((num_users, num_items))\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        scores[user_id, :] = model.predict(user_id, np.arange(num_items), item_features=item_features, num_threads=16)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Compute the predicted scores\n",
    "scores = predict_scores(model, interactions, item_features)\n",
    "print(scores)\n",
    "\n",
    "# Get true relevance scores\n",
    "true_relevance = interactions.toarray()\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "ndcg_5 = ndcg_score(true_relevance, scores, k=5)\n",
    "ndcg_10 = ndcg_score(true_relevance, scores, k=10)\n",
    "\n",
    "print(f'nDCG@5: {ndcg_5}')\n",
    "print(f'nDCG@10: {ndcg_10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OodADKT4EOdJ"
   },
   "outputs": [],
   "source": [
    "df_behaviors_train, df_news_train, vectorizer, user_encoder, news_encoder = process_tsv_files(\"MINDsmall_train/behaviors.tsv\", \"MINDsmall_train/news.tsv\")\n",
    "\n",
    "# need further combine the news_tfidf and history_tfidf\n",
    "df_behaviors_train[\"combined_tfidf\"] = df_behaviors_train.apply(lambda x: np.concatenate([x[\"history_tfidf\"], x[\"news_tfidf\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlUWvcCtyNOy"
   },
   "source": [
    "## Model: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7BJN4ZJDxD2"
   },
   "source": [
    "This model is a Neural Collaborative Filtering (NCF) model designed for recommendation systems. It combines embeddings of users and items, processing them through fully connected layers to predict user preferences for items.\n",
    "\n",
    "#### Main Components\n",
    "\n",
    "1. **User Embedding Layer**:\n",
    "   - Uses `nn.Embedding` to create an embedding vector for each user, capturing user features and preferences.\n",
    "   - `self.user_embedding = nn.Embedding(num_users, embedding_dim)`\n",
    "\n",
    "2. **Item Embedding Layer**:\n",
    "   - Uses `nn.Embedding` to create an embedding vector for each item (news), capturing item features.\n",
    "   - `self.item_embedding = nn.Embedding(num_items, embedding_dim)`\n",
    "\n",
    "3. **History TF-IDF Processing Layer**:\n",
    "   - Uses `nn.Linear` to map the TF-IDF vector of historical news to the embedding dimension.\n",
    "   - `self.history_dense = nn.Linear(tf_vector_length, embedding_dim)`\n",
    "\n",
    "4. **News TF-IDF Processing Layer**:\n",
    "   - Uses `nn.Linear` to map the TF-IDF vector of current news to the embedding dimension.\n",
    "   - `self.news_dense = nn.Linear(tf_vector_length, embedding_dim)`\n",
    "\n",
    "5. **Fully Connected Layers**:\n",
    "   - Combines user embeddings, item embeddings, historical TF-IDF embeddings, and news TF-IDF embeddings, processing them through a series of fully connected layers to predict the click probability.\n",
    "   - `self.fc1 = nn.Linear(embedding_dim * 4, 128)`\n",
    "   - `self.fc2 = nn.Linear(128, 64)`\n",
    "   - `self.fc3 = nn.Linear(64, 1)`\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "During the forward pass, the model processes the input user ID, news ID, historical news TF-IDF vector, and current news TF-IDF vector as follows:\n",
    "\n",
    "1. **Embedding Layer Processing**:\n",
    "   - Obtains the embedding vectors for users and news.\n",
    "   - `user_embeds = self.user_embedding(user_ids)`\n",
    "   - `item_embeds = self.item_embedding(item_ids)`\n",
    "\n",
    "2. **TF-IDF Processing**:\n",
    "   - Uses linear layers to map the historical and current news TF-IDF vectors to the embedding dimension, followed by ReLU activation.\n",
    "   - `history_embeds = F.relu(self.history_dense(history_tfidf))`\n",
    "   - `news_embeds = F.relu(self.news_dense(news_tfidf))`\n",
    "\n",
    "3. **Concatenation and Fully Connected Layer Processing**:\n",
    "   - Concatenates user embeddings, item embeddings, historical TF-IDF embeddings, and news TF-IDF embeddings, processes them through fully connected layers with ReLU activation.\n",
    "   - `x = torch.cat([user_embeds, item_embeds, history_embeds, news_embeds], dim=1)`\n",
    "   - `x = F.relu(self.fc1(x))`\n",
    "   - `x = F.relu(self.fc2(x))`\n",
    "   - Outputs the click probability using the sigmoid function.\n",
    "   - `x = torch.sigmoid(self.fc3(x))`\n",
    "\n",
    "#### Model Prediction\n",
    "- Finally, the model outputs a value between 0 and 1, indicating the probability of the user clicking on the news.\n",
    "\n",
    "By combining user and item embeddings with TF-IDF features, this model can capture the complex relationships between users and items, enhancing the accuracy and effectiveness of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl6uGy5tyNOy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# by default, the vector length is 2000\n",
    "df_behaviors_train, df_news_train, vectorizer, user_encoder, news_encoder = process_tsv_files(\n",
    "                                    \"MINDsmall_train/behaviors.tsv\", \"MINDsmall_train/news.tsv\")\n",
    "\n",
    "df_behaviors_val, df_news_val, vectorizer, user_encoder, news_encoder = process_tsv_files(\n",
    "                                    \"MINDsmall_dev/behaviors.tsv\", \"MINDsmall_dev/news.tsv\",\n",
    "                                    user_encoder=user_encoder, news_encoder=news_encoder, vectorizer=vectorizer)\n",
    "\n",
    "# df_behaviors_test, df_news_test, vectorizer, user_encoder, news_encoder = process_tsv_files(\n",
    "#                                     \"MINDsmall_test/behaviors.tsv\", \"MINDsmall_test/news.tsv\",\n",
    "#                                     user_encoder=user_encoder, news_encoder=news_encoder, vectorizer=vectorizer)\n",
    "\n",
    "# Prepare the dataset\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df_behaviors_train['User_ID'].values, dtype=torch.long)\n",
    "        self.news_ids = torch.tensor(df_behaviors_train['News_ID'].values, dtype=torch.long)\n",
    "        self.history_tfidf = torch.tensor(np.stack(df_behaviors_train['history_tfidf'].values))\n",
    "        self.news_tfidf = torch.tensor(np.stack(df_behaviors_train['news_tfidf'].values))\n",
    "        self.labels = torch.tensor(df_behaviors_train['Click'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.user_ids[idx], self.news_ids[idx], self.history_tfidf[idx], self.news_tfidf[idx], self.labels[idx])\n",
    "\n",
    "train_dataset = RecommendationDataset(df_behaviors_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# validation loader\n",
    "val_dataset = RecommendationDataset(df_behaviors_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# and test\n",
    "# test_dataset = RecommendationDataset(df_behaviors_test)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fni8E2LGDaA5"
   },
   "source": [
    "#### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP8aKV4GDZf3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(self, num_users, num_items, tf_vector_length, embedding_dim=32):\n",
    "        super(NeuralCollaborativeFiltering, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.history_dense = nn.Linear(tf_vector_length, embedding_dim)\n",
    "        self.news_dense = nn.Linear(tf_vector_length, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, history_tfidf, news_tfidf):\n",
    "        user_embeds = self.user_embedding(user_ids)\n",
    "        item_embeds = self.item_embedding(item_ids)\n",
    "        history_embeds = F.relu(self.history_dense(history_tfidf))\n",
    "        news_embeds = F.relu(self.news_dense(news_tfidf))\n",
    "        x = torch.cat([user_embeds, item_embeds, history_embeds, news_embeds], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXTOgZ8jDg6O"
   },
   "source": [
    "For training, the Adam optimizer is used, and for loss, the binrary cross entropy loss is used (because it predicts 1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEuq0YG9Dipc"
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "class TrainModelClass(L.LightningModule):\n",
    "    def __init__(self, model, criterion, lr=1e-3):\n",
    "        super(TrainModelClass, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, user_ids, item_ids, history_tfidf, news_tfidf):\n",
    "        return self.model(user_ids, item_ids, history_tfidf, news_tfidf)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_ids, item_ids, history_tfidf, news_tfidf, labels = batch\n",
    "\n",
    "        user_ids = user_ids.long()\n",
    "        item_ids = item_ids.long()\n",
    "        history_tfidf = history_tfidf.float()\n",
    "        news_tfidf = news_tfidf.float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        outputs = self(user_ids, item_ids, history_tfidf, news_tfidf).squeeze()\n",
    "        loss = self.criterion(outputs, labels.float())\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def val_test_step(self, batch, batch_idx):\n",
    "        user_ids, item_ids, history_tfidf, news_tfidf, labels = batch\n",
    "        user_ids = user_ids.long()\n",
    "        item_ids = item_ids.long()\n",
    "        history_tfidf = history_tfidf.float()\n",
    "        news_tfidf = news_tfidf.float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        outputs = self(user_ids, item_ids, history_tfidf, news_tfidf).squeeze()\n",
    "        self.prob_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        # # each user is exposed to user_ids, history_tfidf, then the news_tfidf\n",
    "        # # so need to save the mapping with key = (user_id, history_tfidf) and value = output\n",
    "        # for user_id, history, output, label in zip(user_ids, history_tfidf, outputs, labels):\n",
    "        #     # if the user_id and history is not in the dictionary, then add it\n",
    "        #     if (user_id.item(), tuple(history.tolist())) not in self.user_history_session:\n",
    "        #         self.user_history_session[(user_id.item(), tuple(history.tolist()))] = []\n",
    "        #     self.user_history_session[(user_id.item(), tuple(history.tolist()))].append((output.item(), label.item()))\n",
    "\n",
    "        outputs = torch.round(outputs)\n",
    "\n",
    "        # save the predictions and labels\n",
    "        self.preds.extend(outputs.detach().cpu().numpy())\n",
    "        self.labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "        self.prob_preds = []\n",
    "        self.user_history_session = {}\n",
    "\n",
    "    # for the validation step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.val_test_step(batch, batch_idx)\n",
    "\n",
    "    # for the validation epoch end\n",
    "    def on_validation_epoch_end(self):\n",
    "        # calculate the auc\n",
    "        val_auc = roc_auc_score(self.labels, self.preds)\n",
    "        self.log(\"val_auc\", val_auc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # is it possible to calculate the nDCG@5 and nDCG@10?\n",
    "        # calculate the nDCG@5 and nDCG@10\n",
    "        # val_ndcg_5 = ndcg_score(self.val_labels, self.val_prob_preds, k=5)\n",
    "        #\n",
    "\n",
    "    # for the test step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.val_test_step(batch, batch_idx)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_auc = roc_auc_score(self.labels, self.preds)\n",
    "        self.log(\"test_auc\", test_auc, on_step=False, on_epoch=True, prog_bar=True, logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orcLdIKrDkCC",
    "outputId": "9c39290d-6573-47d6-e6b7-3bf43b43410d"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(news_encoder.classes_)\n",
    "model = NeuralCollaborativeFiltering(num_users, num_items, 2000)\n",
    "\n",
    "# wrap the model with the TrainModelClass\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "model = TrainModelClass(model, loss_fn, lr=1e-3)\n",
    "\n",
    "# set up the trainer\n",
    "csv_logger = L.pytorch.loggers.CSVLogger(\"lightning_logs\", name=\"ncf\")\n",
    "trainer = L.Trainer(max_epochs=10, logger=csv_logger)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-FU0zbkDrlt",
    "outputId": "770c88b7-14c1-4f6f-d4de-35ec231addf7"
   },
   "outputs": [],
   "source": [
    "# plot the line with train_loss, and another graph with val_auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"lightning_logs/ncf/version_0/metrics.csv\")\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNnJ8weQDs_L",
    "outputId": "c3df7d37-b723-4b1d-e4cf-30f808e976a8"
   },
   "outputs": [],
   "source": [
    "# for the train_loss column, some values may be NaN\n",
    "train_loss_values = df['train_loss'].dropna().values\n",
    "val_auc_values = df['val_auc'].dropna().values\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axs[0].plot(train_loss_values)\n",
    "axs[1].plot(val_auc_values)\n",
    "\n",
    "axs[0].set_title(\"Neural Collaborative Filtering: Train Loss\")\n",
    "axs[1].set_title(\"Neural Collaborative Filtering: Validation AUC\")\n",
    "\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "\n",
    "axs[0].set_ylabel(\"Train Loss\")\n",
    "axs[1].set_ylabel(\"Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQVvfemNSZQT"
   },
   "source": [
    "## Model: Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDhqwTYX-lFa"
   },
   "source": [
    "### Libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN0QdC4C7Ubx",
    "outputId": "744422ad-0ce5-4bba-a388-c7a6bbdfff87"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0sSlq8o25aa",
    "outputId": "97503df2-8102-4791-8215-346b16430c7e"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT4R2Pmx7YMP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9UqxNoD-gxQ"
   },
   "source": [
    "### view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "fL10s53Z7d3I",
    "outputId": "4fd69373-7857-424c-be70-c13ebbc92474"
   },
   "outputs": [],
   "source": [
    "raw_behaviour = pd.read_csv(\"/content/drive/My Drive/ColabFiles_9727data/behaviors.tsv\",sep=\"\\t\",names=[\"impressionId\",\"userId\",\"timestamp\",\"click_history\",\"impressions\"])\n",
    "\n",
    "print(f\"The dataset originally consist of {len(raw_behaviour)} number of interactions.\")\n",
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "BbG33PET8V_K",
    "outputId": "206eee56-9cb6-4b29-f08a-15cb6703a73c"
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"/content/drive/My Drive/ColabFiles_9727data/news.tsv\",sep=\"\\t\",names=[\"itemId\",\"category\",\"subcategory\",\"title\",\"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n",
    "print(f\"The article data consist in total of {len(news)} number of articles.\")\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlAyDYV967jE",
    "outputId": "f56d8d8e-ab76-467b-8465-f9393be3aa12"
   },
   "outputs": [],
   "source": [
    "# Print the number of interactions\n",
    "print(f\"The dataset originally consists of {len(raw_behaviour)} interactions.\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(raw_behaviour.head())\n",
    "\n",
    "# Check how many unique values are in a specific column, for example, \"userId\"\n",
    "unique_user_ids = raw_behaviour['userId'].nunique()\n",
    "print(f\"The number of unique userId values is: {unique_user_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CCvBb2f7LBT",
    "outputId": "9563a88e-b225-4a2b-b739-a13f05a54e0d"
   },
   "outputs": [],
   "source": [
    "# Print the number of interactions\n",
    "print(f\"The dataset originally consists of {len(raw_behaviour)} interactions.\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(raw_behaviour.head())\n",
    "\n",
    "# Check if a specific column, for example 'userId', contains any missing or empty values\n",
    "missing_user_ids = raw_behaviour['click_history'].isnull().sum()\n",
    "empty_user_ids = (raw_behaviour['click_history'] == '').sum()\n",
    "\n",
    "print(f\"The 'click_history' column contains {missing_user_ids} missing values.\")\n",
    "print(f\"The 'click_history' column contains {empty_user_ids} empty values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrZQd5cS88nQ"
   },
   "outputs": [],
   "source": [
    "# Function to split the impressions and clicks into two seperate lists\n",
    "def process_impression(impression_list):\n",
    "    list_of_strings = impression_list.split()\n",
    "    click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '1']\n",
    "    non_click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '0']\n",
    "    return click,non_click\n",
    "\n",
    "# Indexize these two new columns:\n",
    "raw_behaviour['click'], raw_behaviour['noclicks'] = zip(*raw_behaviour['impressions'].map(process_impression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5usKvPj89CaI"
   },
   "outputs": [],
   "source": [
    "# Convert timestamp value to hours since epoch\n",
    "raw_behaviour['epochhrs'] = pd.to_datetime(raw_behaviour['timestamp']).values.astype(np.int64)/(1e6)/1000/3600\n",
    "raw_behaviour['epochhrs'] = raw_behaviour['epochhrs'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3cwRXl29Dhj",
    "outputId": "db4655ff-6331-469b-8328-6c85093ff399"
   },
   "outputs": [],
   "source": [
    "# If there exists several clicks in one session, expand to new observation\n",
    "raw_behaviour = raw_behaviour.explode(\"click\").reset_index(drop=True)\n",
    "\n",
    "# Extract the clicks from the previous clicks\n",
    "click_history = raw_behaviour[[\"userId\",\"click_history\"]].drop_duplicates().dropna()\n",
    "click_history[\"click_history\"] = click_history.click_history.map(lambda x: x.split())\n",
    "click_history = click_history.explode(\"click_history\").rename(columns={\"click_history\":\"click\"})\n",
    "# Dummy time set to earlies epochhrs in raw_behaviour as we don't know when these events took place.\n",
    "click_history[\"epochhrs\"] = raw_behaviour.epochhrs.min()\n",
    "click_history[\"noclicks\"] = pd.Series([[] for _ in range(len(click_history.index))])\n",
    "\n",
    "# concatenate historical clicks with the raw_behaviour\n",
    "raw_behaviour = pd.concat([raw_behaviour,click_history],axis=0).reset_index(drop=True)\n",
    "print(f\"The dataset after pre-processing consist of {len(raw_behaviour)} number of interactions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aUJJ7dkHu2A"
   },
   "source": [
    "### visualize the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "_NoeLufD9Dt8",
    "outputId": "4a25f6d8-2634-42f7-ae77-f24d1d0a494d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "article_clicks = raw_behaviour['click'].explode().value_counts()\n",
    "\n",
    "filtered_clicks = article_clicks[article_clicks <= 100]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(filtered_clicks, bins=50, kde=False)\n",
    "plt.title('Distribution of Article Clicks (<= 100 clicks)')\n",
    "plt.xlabel('Number of Clicks')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n",
    "\n",
    "filtered_clicks = article_clicks[article_clicks <= 50]\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(filtered_clicks, bins=50, kde=False)\n",
    "plt.title('Distribution of Article Clicks (<= 50 clicks)')\n",
    "plt.xlabel('Number of Clicks')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPF0YWgxH0Ag"
   },
   "source": [
    "#### Because the clicks are nearly almost 70% of them less than 10, we set the cutoff be 5, to try maintain as much users as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5qa93yBZVPh",
    "outputId": "f8c0182f-aeff-48f1-81a9-e766d9feea71"
   },
   "outputs": [],
   "source": [
    "min_click_cutoff = 5\n",
    "print(f'Number of items that have less than {min_click_cutoff} clicks make up',np.round(np.mean(raw_behaviour.groupby(\"click\").size() < min_click_cutoff)*100,3),'% of the total, and these will be removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydlmejw59D4w"
   },
   "outputs": [],
   "source": [
    "# remove items with less clicks than min_click_cutoff\n",
    "raw_behaviour = raw_behaviour[raw_behaviour.groupby(\"click\")[\"userId\"].transform('size') >= min_click_cutoff].reset_index(drop=True)\n",
    "# Get a set with all the unique items\n",
    "click_set = set(raw_behaviour['click'].unique())\n",
    "\n",
    "# remove items for impressions that is not avaiable in the click set (the items that we will be training on)\n",
    "raw_behaviour['noclicks'] = raw_behaviour['noclicks'].apply(lambda impressions: [impression for impression in impressions if impression in click_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "-WSZrGfxZvLX",
    "outputId": "9abf6238-3668-44fe-9f7b-da8eeb7c37b0"
   },
   "outputs": [],
   "source": [
    "## Select the columns that we now want to use for further analysis\n",
    "behaviour = raw_behaviour[['epochhrs','userId','click','noclicks']].copy()\n",
    "\n",
    "print('Number of interactions in the behaviour dataset:', behaviour.shape[0])\n",
    "print('Number of users in the behaviour dataset:', behaviour.userId.nunique())\n",
    "print('Number of articles in the behaviour dataset:', behaviour.click.nunique())\n",
    "\n",
    "behaviour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyn6JxS0IsfD"
   },
   "source": [
    "### We can split the data into 80 training and 20 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvYyo-RHjrW8",
    "outputId": "7a97c431-7a65-46b2-9924-bc15cbf2da7b"
   },
   "outputs": [],
   "source": [
    "# Let us use the last 10pct of the data as our validation data:\n",
    "test_time_th = behaviour['epochhrs'].quantile(0.8)\n",
    "train = behaviour[behaviour['epochhrs']< test_time_th].copy()\n",
    "\n",
    "## Indexize items\n",
    "# Allocate a unique index for each item, but let the zeroth index be a UNK index:\n",
    "ind2item = {idx +1: itemid for idx, itemid in enumerate(train.click.unique())}\n",
    "item2ind = {itemid : idx for idx, itemid in ind2item.items()}\n",
    "\n",
    "train['noclicks'] = train['noclicks'].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "train['click'] = train['click'].map(lambda item: item2ind.get(item, 0))\n",
    "\n",
    "## Indexize users\n",
    "# Allocate a unique index for each user, but let the zeroth index be a UNK index:\n",
    "ind2user = {idx +1: userid for idx, userid in enumerate(train['userId'].unique())}\n",
    "user2ind = {userid : idx for idx, userid in ind2user.items()}\n",
    "\n",
    "# Create a new column with userIdx:\n",
    "train['userIdx'] = train['userId'].map(lambda x: user2ind.get(x,0))\n",
    "\n",
    "# Repeat for validation\n",
    "valid =  behaviour[behaviour['epochhrs']>= test_time_th].copy()\n",
    "valid[\"click\"] = valid[\"click\"].map(lambda item: item2ind.get(item, 0))\n",
    "valid[\"noclicks\"] = valid[\"noclicks\"].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "valid[\"userIdx\"] = valid[\"userId\"].map(lambda x: user2ind.get(x,0))\n",
    "\n",
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ua3oeLKR7l1"
   },
   "source": [
    "### New splitting way: 70 training, 15 validation. 15 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpihmQvYSCnH"
   },
   "outputs": [],
   "source": [
    "# Split into 70% training set, 15% validation set, 15% test set\n",
    "test_time_th = behaviour['epochhrs'].quantile(0.85)\n",
    "valid_time_th = behaviour['epochhrs'].quantile(0.7)\n",
    "train = behaviour[behaviour['epochhrs']< valid_time_th].copy()\n",
    "\n",
    "\n",
    "## Indexize items\n",
    "# Allocate a unique index for each item, but let the zeroth index be a UNK index:\n",
    "ind2item = {idx +1: itemid for idx, itemid in enumerate(train.click.unique())}\n",
    "item2ind = {itemid : idx for idx, itemid in ind2item.items()}\n",
    "\n",
    "train['noclicks'] = train['noclicks'].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "train['click'] = train['click'].map(lambda item: item2ind.get(item, 0))\n",
    "\n",
    "## Indexize users\n",
    "# Allocate a unique index for each user, but let the zeroth index be a UNK index:\n",
    "ind2user = {idx +1: userid for idx, userid in enumerate(train['userId'].unique())}\n",
    "user2ind = {userid : idx for idx, userid in ind2user.items()}\n",
    "\n",
    "# Create a new column with userIdx:\n",
    "train['userIdx'] = train['userId'].map(lambda x: user2ind.get(x,0))\n",
    "\n",
    "# Repeat for validation\n",
    "valid = behaviour[(behaviour['epochhrs'] >= valid_time_th) & (behaviour['epochhrs'] < test_time_th)].copy()\n",
    "valid[\"click\"] = valid[\"click\"].map(lambda item: item2ind.get(item, 0))\n",
    "valid[\"noclicks\"] = valid[\"noclicks\"].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "valid[\"userIdx\"] = valid[\"userId\"].map(lambda x: user2ind.get(x,0))\n",
    "\n",
    "# for test\n",
    "test = behaviour[behaviour['epochhrs'] >= test_time_th].copy()\n",
    "test['click'] = test['click'].map(lambda item: item2ind.get(item, 0))\n",
    "test['noclicks'] = test['noclicks'].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "test['userIdx'] = test['userId'].map(lambda x: user2ind.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv5hT9Z9kRcb"
   },
   "outputs": [],
   "source": [
    "class MindDataset(Dataset):\n",
    "    # A fairly simple torch dataset module that can take a pandas dataframe (as above),\n",
    "    # and convert the relevant fields into a dictionary of arrays that can be used in a dataloader\n",
    "    def __init__(self, df):\n",
    "        # Create a dictionary of tensors out of the dataframe\n",
    "        self.data = {\n",
    "            'userIdx' : torch.tensor(df.userIdx.values.astype(np.int64)),\n",
    "            'click' : torch.tensor(df.click.values.astype(np.int64))\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data['userIdx'])\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGUbQnM5SIqk"
   },
   "outputs": [],
   "source": [
    "# Build datasets and dataloaders of train and validation dataframes:\n",
    "bs = 1024\n",
    "ds_train = MindDataset(train)\n",
    "train_loader = DataLoader(ds_train, batch_size=bs, shuffle=True)\n",
    "ds_valid = MindDataset(valid)\n",
    "valid_loader = DataLoader(ds_valid, batch_size=bs, shuffle=False)\n",
    "ds_test = MindDataset(test)\n",
    "test_loader = DataLoader(ds_test, batch_size=bs, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ec7-kz-AnK3O",
    "outputId": "8b8d85c9-3d8a-4cea-98c4-b2ae4da64c03"
   },
   "outputs": [],
   "source": [
    "# View the first batch in the test loader\n",
    "batch = next(iter(test_loader))\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufldvb9soPsJ",
    "outputId": "62476b08-3d24-4e90-bf78-87b20b2657ae"
   },
   "outputs": [],
   "source": [
    "# Alternatively, iterate through the test loader to view all batches\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(batch)\n",
    "    # Break after the first batch for brevity\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwozUGFKkSD6"
   },
   "outputs": [],
   "source": [
    "# Build a matrix factorization model\n",
    "class NewsMF(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, dim = 100, dropout_prob=0.2, reg=0.01): # add regularization\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.reg = reg\n",
    "        self.useremb = nn.Embedding(num_embeddings=num_users, embedding_dim=dim)\n",
    "        self.itememb = nn.Embedding(num_embeddings=num_items, embedding_dim=dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob) # the drop out probablity is set to 0.2\n",
    "\n",
    "\n",
    "    def step(self, batch, batch_idx, phase=\"train\"):\n",
    "        batch_size = batch['userIdx'].size(0)\n",
    "        uservec = self.useremb(batch['userIdx'])\n",
    "        itemvec_click = self.itememb(batch['click'])\n",
    "\n",
    "        # Apply dropout to embeddings\n",
    "        uservec = self.dropout(uservec)                # added drop out\n",
    "        itemvec_click = self.dropout(itemvec_click)\n",
    "\n",
    "        # For each positive interaction,sample a random negative\n",
    "        neg_sample = torch.randint_like(batch[\"click\"],1,self.num_items)\n",
    "        itemvec_noclick = self.itememb(neg_sample)\n",
    "        itemvec_noclick = self.dropout(itemvec_noclick)  # Apply dropout to negative samples\n",
    "\n",
    "        score_click = torch.sigmoid((uservec*itemvec_click).sum(-1).unsqueeze(-1))\n",
    "        score_noclick =  torch.sigmoid((uservec*itemvec_noclick).sum(-1).unsqueeze(-1))\n",
    "\n",
    "        # Compute loss as binary cross entropy (categorical distribution between the clicked and the no clicked item)\n",
    "        scores_all = torch.concat((score_click, score_noclick), dim=1)\n",
    "        target_all = torch.concat((torch.ones_like(score_click), torch.zeros_like(score_noclick)),dim=1)\n",
    "        # loss = F.binary_cross_entropy(scores_all, target_all)\n",
    "        # return loss\n",
    "        loss = F.binary_cross_entropy(scores_all, target_all)\n",
    "        reg_loss = self.reg * (self.useremb.weight.norm(2) + self.itememb.weight.norm(2)) # add regularization\n",
    "        return loss + reg_loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # for now, just do the same computation as during training\n",
    "        return self.step(batch, batch_idx, \"val\")\n",
    "\n",
    "    #def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        #return optimizer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf1AU2hNkSHd"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362,
     "referenced_widgets": [
      "82912480b5ba4a6c9b3aaff5a3796785",
      "0ec278a7e10049a8a2710f30577fc5cc",
      "10022dada02d409f9bb2a36f264afeb2",
      "fe1eb178cad94636937e5e7761b3ebe6",
      "6edff786206346f7802cf4a0640aa9de",
      "d9b0714ac16646bcb0c71a08c61922fe",
      "8a9b9075a89e42789d35a6a9b8687341",
      "058b5032e9424c4b828d70331e1e4e55",
      "edc1243c08fb4eb3bd264298cf2762e3",
      "17a8614103a148fdbe65a4b4354bded7",
      "046443066f3e4759821a538605411f14"
     ]
    },
    "id": "_JQHWYChk5mY",
    "outputId": "90ceb608-4002-4945-eabe-f59551ad1b29"
   },
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "# Define and train model\n",
    "mf_model = NewsMF(num_users=len(ind2user) + 1, num_items = len(ind2item) + 1, dim = 100) # increase the embedding demension to 100\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator=\"gpu\",deterministic = True)\n",
    "trainer.fit(model=mf_model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOSZuZzRUMg7"
   },
   "source": [
    "### New training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "98ad347e62884228bbfece8070bda17c",
      "87b8862486a24472870d042ef0fed611",
      "c871bb77a33949d394936866e014f75b",
      "3fc0220b7d644266b804298715a770fa",
      "27164d2bebc2480bb5a6b6fa37e97ebe",
      "927eaf2e77124db984574f7dcfec7463",
      "f83f7b8d294b47c4be0326cbe43e2484",
      "bdc9761709614c4db31805ad3402bd4c",
      "a5551662f08c43bdbd4c9e3f72612498",
      "4bb763dace3947bb949bd2b531abecba",
      "617bf8d9462843ce8adace1e0e8170e5",
      "d17204f6a8694c97846baeb68f6dac71",
      "63a26081fa0b4644b35def4d27dcbf58",
      "1668f80d06ec436db92c378aef580fc0",
      "9179ed87dccf448883a227d100f35130",
      "11fd5b7bbcb840e4b06c5150be4707de",
      "d2bb796a675144eda6a59443e48d1f16",
      "4baeddea10b348129ad9cbdf16779768",
      "ab4ea60ae08f4ff38c723eaa4400bcc1",
      "c446051456a54937abbe66ab8f1ca8a3",
      "7889201585724968b394c65637a7d1e5",
      "ce804b6ee6f5421da5cb2ce0aa803937",
      "713dc0fa80dd45c9b72046fd520fac6c",
      "e85d3f5cd3324ff39c998e1832263845",
      "7502b430fb864c06bf64d6950f99f291",
      "85db6002de154a51976f77d144bcaa99",
      "c18dec20afb949fa918d44fecb76116d",
      "0185fb7e78ab4851a87cdc0790f0c664",
      "81ff0cad30cf4cd4bc86a78c4ec3fb29",
      "654d84025556496c8ae743b77d696d31",
      "113ea2980863474d9dba43eca4086415",
      "7e7344617a0541a1b7b138ec72de40ba",
      "39dd555dc7ba4dd2af5f288b653a85ca",
      "7fccc90a4e5a4af2a2bf7e12e7cacd94",
      "3b093440267043728d765d88948aea3f",
      "06d95c98c12946ca9ddd1aceb6f6dd21",
      "8a6d31d6e40a442ba4a400ac24eab010",
      "a6025ae985814b9f84bc857fec12cecd",
      "1e2e9a764b6542b5ae049a1bcb77db58",
      "4d0910e3a4a7425ea151ba3934d23b52",
      "1ddf5e2843ea47c3b4362dac48fd5d1c",
      "108c516144e748bc96f7f26ceadb4665",
      "39f250dbae3e438eb9eddeb1f6ff474f",
      "0e962740d2014c68a6e0dd1b18294536"
     ]
    },
    "id": "B-fBt5aDUK8D",
    "outputId": "41f3aa2b-d259-4821-daad-769b75f923a0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define and train the model\n",
    "mf_model = NewsMF(num_users=len(ind2user) + 1, num_items=len(ind2item) + 1, dim=100, dropout_prob=0.2, reg=0.01)\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator=\"gpu\", deterministic=True)\n",
    "trainer.fit(model=mf_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "'''\n",
    "seed_everything(42, workers=True)\n",
    "# Train the model\n",
    "mf_model = NewsMF(num_users=len(ind2user) + 1, num_items=len(ind2item) + 1, dim=100, dropout_prob=0.2, reg=0.01)  # increase the embedding dimension to 100\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator=\"gpu\", deterministic=True)\n",
    "trainer.fit(model=mf_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "VYD2XQ-Ak5xC",
    "outputId": "f0169572-5405-4954-d94f-16d8ed50652d"
   },
   "outputs": [],
   "source": [
    "## Add more information to the article data\n",
    "# The item index\n",
    "news[\"ind\"] = news[\"itemId\"].map(item2ind)\n",
    "news = news.sort_values(\"ind\").reset_index(drop=True)\n",
    "# Number of clicks in training data per article, investigate the cold start issue\n",
    "news[\"n_click_training\"] = news[\"ind\"].map(dict(Counter(train.click)))\n",
    "# 5 most clicked articles\n",
    "news.sort_values(\"n_click_training\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg917uINuwWE"
   },
   "source": [
    "### Test for the most 5 similar news article for a news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HyGK43lTk50S",
    "outputId": "f88a7276-a22c-4958-8bf0-02b38aef029b"
   },
   "outputs": [],
   "source": [
    "# store the learned item embedding into a seperate tensor\n",
    "itememb = mf_model.itememb.weight.detach()\n",
    "print(itememb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "ro5Wug6xk53J",
    "outputId": "92081db5-aa51-4c4c-fcaa-02b1a69b6263"
   },
   "outputs": [],
   "source": [
    "# Investigate different rows of the item embedding (articles embeddings) to see if the model works\n",
    "## some examples N13259, N16636, N10272\n",
    "## Can you find some examples that does not work good? Why?\n",
    "\n",
    "ind = item2ind.get(\"N3259\")\n",
    "# This calculates the cosine similarity and outputs the 5 most similar articles w.r.t to ind in descending order\n",
    "similarity = torch.nn.functional.cosine_similarity(itememb[ind], itememb, dim=1)\n",
    "most_sim = news[~news.ind.isna()].iloc[(similarity.argsort(descending=True).numpy()-1)]\n",
    "most_sim.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iuBBV8rUz2q"
   },
   "source": [
    "### Calculate the new prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7csWBrPU5fV"
   },
   "source": [
    "### New recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCNeaJGU9A4",
    "outputId": "dd175d66-8a78-4765-eadf-fd09ea12ddac"
   },
   "outputs": [],
   "source": [
    "# Function to recommend news articles to a user\n",
    "def recommend_news(user_idx, model, top_k=5):\n",
    "    user_vector = model.useremb(torch.tensor(user_idx))\n",
    "    scores = torch.sigmoid((user_vector * model.itememb.weight).sum(-1))\n",
    "    top_k_items = scores.argsort(descending=True)[:top_k]\n",
    "    return [ind2item[idx.item()] for idx in top_k_items]\n",
    "\n",
    "# Example: Recommend top 5 news articles for a user\n",
    "user_id = test['userIdx'].iloc[3616]  # Replace with the desired user index\n",
    "recommended_news = recommend_news(user_id, mf_model, top_k=5)\n",
    "print(f\"Recommended news articles for user {user_id}: {recommended_news}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test['recommended'] = test['userIdx'].apply(lambda x: recommend_news(x, mf_model, top_k=5))\n",
    "\n",
    "print(test.head(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nw6I67xU3m_",
    "outputId": "1dbe5f17-340c-4021-e3bd-9167ae8edb48"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and calculate predictions\n",
    "def calculate_prediction(user_idx, item_idx):\n",
    "    user_tensor = torch.tensor(user_idx, dtype=torch.long)\n",
    "    item_tensor = torch.tensor(item_idx, dtype=torch.long)\n",
    "    user_vector = mf_model.useremb(user_tensor)\n",
    "    item_vector = mf_model.itememb(item_tensor)\n",
    "    score = torch.sigmoid((user_vector * item_vector).sum(-1))\n",
    "    return score.item()\n",
    "\n",
    "test['pred'] = test.apply(lambda x: calculate_prediction(x['userIdx'], x['click']), axis=1)\n",
    "print(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weB3sMZa5hc9"
   },
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGzL-yoikjwL",
    "outputId": "ce934684-1936-4b77-9d7e-d71ad4b87499"
   },
   "outputs": [],
   "source": [
    "unique_values = test['click'].unique()\n",
    "print(f\"Unique values in 'click': {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS7b3FHOUYHD"
   },
   "source": [
    "### MF2 (with precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjhKsLttUD2I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torchmetrics.functional import auroc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVjTwrb5UD2K",
    "outputId": "54914442-cea3-4fc8-a548-372eda28551f"
   },
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "raw_behaviour = pd.read_csv(\n",
    "    os.path.join(data_path, \"behaviors.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    names=[\"impressionId\",\"userId\",\"timestamp\",\"click_history\",\"impressions\"])\n",
    "\n",
    "print(f\"The dataset originally consist of {len(raw_behaviour)} number of interactions.\")\n",
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFzngrrqUD2L",
    "outputId": "e1226550-852e-4c72-c120-c4dca30265ae"
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(\n",
    "    os.path.join(data_path,\"news.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    names=[\"itemId\",\"category\",\"subcategory\",\"title\",\"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n",
    "print(f\"The article data consist in total of {len(news)} number of articles.\")\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRFOi8w9UD2M"
   },
   "outputs": [],
   "source": [
    "def process_impression(impression_list):\n",
    "    list_of_strings = impression_list.split()\n",
    "    click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '1']\n",
    "    non_click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '0']\n",
    "    return click,non_click\n",
    "\n",
    "# We can then indexize these two new columns:\n",
    "raw_behaviour['click'], raw_behaviour['noclicks'] = zip(*raw_behaviour['impressions'].map(process_impression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYPXdd8jUD2M"
   },
   "outputs": [],
   "source": [
    "raw_behaviour['epochhrs'] = pd.to_datetime(raw_behaviour['timestamp']).values.astype(np.int64)/(1e6)/1000/3600\n",
    "raw_behaviour['epochhrs'] = raw_behaviour['epochhrs'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSSfZRifUD2M",
    "outputId": "34c201c0-7ced-4cd4-b930-73684c6857ba"
   },
   "outputs": [],
   "source": [
    "raw_behaviour = raw_behaviour.explode(\"click\").reset_index(drop=True)\n",
    "\n",
    "# Extract the clicks from the previous clicks\n",
    "click_history = raw_behaviour[[\"userId\",\"click_history\"]].drop_duplicates().dropna()\n",
    "click_history[\"click_history\"] = click_history.click_history.map(lambda x: x.split())\n",
    "click_history = click_history.explode(\"click_history\").rename(columns={\"click_history\":\"click\"})\n",
    "# Dummy time set to earlies epochhrs in raw_behaviour as we don't know when these events took place.\n",
    "click_history[\"epochhrs\"] = raw_behaviour.epochhrs.min()\n",
    "click_history[\"noclicks\"] = pd.Series([[] for _ in range(len(click_history.index))])\n",
    "\n",
    "# concatenate historical clicks with the raw_behaviour\n",
    "raw_behaviour = pd.concat([raw_behaviour,click_history],axis=0).reset_index(drop=True)\n",
    "print(f\"The dataset after pre-processing consist of {len(raw_behaviour)} number of interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40dmXSdhUD2N",
    "outputId": "e29133f9-954f-413b-ab45-d43e44c4f2be"
   },
   "outputs": [],
   "source": [
    "min_click_cutoff = 100\n",
    "print(f'Number of items that have less than {min_click_cutoff} clicks make up',np.round(np.mean(raw_behaviour.groupby(\"click\").size() < min_click_cutoff)*100,3),'% of the total, and these will be removed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAobgVaZUD2N"
   },
   "outputs": [],
   "source": [
    "raw_behaviour = raw_behaviour[raw_behaviour.groupby(\"click\")[\"userId\"].transform('size') >= min_click_cutoff].reset_index(drop=True)\n",
    "# Get a set with all the unique items\n",
    "click_set = set(raw_behaviour['click'].unique())\n",
    "\n",
    "# remove items for impressions that is not avaiable in the click set (the items that we will be training on)\n",
    "raw_behaviour['noclicks'] = raw_behaviour['noclicks'].apply(lambda impressions: [impression for impression in impressions if impression in click_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSQRVvnhUD2N",
    "outputId": "b4eff9b4-b439-450c-f209-6cf536c9fc46"
   },
   "outputs": [],
   "source": [
    "behaviour = raw_behaviour[['epochhrs','userId','click','noclicks']].copy()\n",
    "\n",
    "print('Number of interactions in the behaviour dataset:', behaviour.shape[0])\n",
    "print('Number of users in the behaviour dataset:', behaviour.userId.nunique())\n",
    "print('Number of articles in the behaviour dataset:', behaviour.click.nunique())\n",
    "\n",
    "behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvqZoVlfUD2O"
   },
   "outputs": [],
   "source": [
    "test_time_th = behaviour['epochhrs'].quantile(0.9)\n",
    "train = behaviour[behaviour['epochhrs']< test_time_th].copy()\n",
    "\n",
    "## Indexize items\n",
    "# Allocate a unique index for each item, but let the zeroth index be a UNK index:\n",
    "ind2item = {idx +1: itemid for idx, itemid in enumerate(train.click.unique())}\n",
    "item2ind = {itemid : idx for idx, itemid in ind2item.items()}\n",
    "\n",
    "train['noclicks'] = train['noclicks'].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "train['click'] = train['click'].map(lambda item: item2ind.get(item, 0))\n",
    "\n",
    "## Indexize users\n",
    "# Allocate a unique index for each user, but let the zeroth index be a UNK index:\n",
    "ind2user = {idx +1: userid for idx, userid in enumerate(train['userId'].unique())}\n",
    "user2ind = {userid : idx for idx, userid in ind2user.items()}\n",
    "\n",
    "# Create a new column with userIdx:\n",
    "train['userIdx'] = train['userId'].map(lambda x: user2ind.get(x,0))\n",
    "\n",
    "# Repeat for validation\n",
    "valid =  behaviour[behaviour['epochhrs']>= test_time_th].copy()\n",
    "valid[\"click\"] = valid[\"click\"].map(lambda item: item2ind.get(item, 0))\n",
    "valid[\"noclicks\"] = valid[\"noclicks\"].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n",
    "valid[\"userIdx\"] = valid[\"userId\"].map(lambda x: user2ind.get(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4nQafk8UD2P"
   },
   "outputs": [],
   "source": [
    "class MindDataset(Dataset):\n",
    "    # A fairly simple torch dataset module that can take a pandas dataframe (as above),\n",
    "    # and convert the relevant fields into a dictionary of arrays that can be used in a dataloader\n",
    "    def __init__(self, df):\n",
    "        # Create a dictionary of tensors out of the dataframe\n",
    "        self.data = {\n",
    "            'userIdx' : torch.tensor(df.userIdx.values.astype(np.int64)),\n",
    "            'click' : torch.tensor(df.click.values.astype(np.int64))\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data['userIdx'])\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGZMBOQKUD2P"
   },
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "ds_train = MindDataset(train)\n",
    "train_loader = DataLoader(ds_train, batch_size=bs, shuffle=True)\n",
    "ds_valid = MindDataset(valid)\n",
    "valid_loader = DataLoader(ds_valid, batch_size=bs, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQcnSC4XUD2P"
   },
   "outputs": [],
   "source": [
    "# Build a matrix factorization model\n",
    "class NewsMF(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, dim = 10):\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.useremb = nn.Embedding(num_embeddings=num_users, embedding_dim=dim)\n",
    "        self.itememb = nn.Embedding(num_embeddings=num_items, embedding_dim=dim)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_vec = self.useremb(user_idx)\n",
    "        item_vec = self.itememb(item_idx)\n",
    "        dot_product = (user_vec * item_vec).sum(-1).unsqueeze(-1)\n",
    "        score = torch.sigmoid(dot_product)\n",
    "        return score\n",
    "\n",
    "    def step(self, batch, batch_idx, phase=\"train\"):\n",
    "        batch_size = batch['userIdx'].size(0)\n",
    "        uservec = self.useremb(batch['userIdx'])\n",
    "        itemvec_click = self.itememb(batch['click'])\n",
    "\n",
    "        # For each positive interaction,sample a random negative\n",
    "        neg_sample = torch.randint_like(batch[\"click\"],1,self.num_items)\n",
    "        itemvec_noclick = self.itememb(neg_sample)\n",
    "\n",
    "        score_click = torch.sigmoid((uservec*itemvec_click).sum(-1).unsqueeze(-1))\n",
    "        score_noclick =  torch.sigmoid((uservec*itemvec_noclick).sum(-1).unsqueeze(-1))\n",
    "\n",
    "        # Compute loss as binary cross entropy (categorical distribution between the clicked and the no clicked item)\n",
    "        scores_all = torch.concat((score_click, score_noclick), dim=1)\n",
    "        target_all = torch.concat((torch.ones_like(score_click), torch.zeros_like(score_noclick)),dim=1)\n",
    "        loss = F.binary_cross_entropy(scores_all, target_all)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # for now, just do the same computation as during training\n",
    "        return self.step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXw5VvWgUD2Q"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj1-aHu8UD2Q",
    "outputId": "4b9e6c1f-f930-454c-ed35-3dab9cac855a"
   },
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "# Define and train model\n",
    "mf_model = NewsMF(num_users=len(ind2user) + 1, num_items = len(ind2item) + 1, dim = 50)\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator=\"gpu\",deterministic = True)\n",
    "trainer.fit(model=mf_model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XccgylVhUD2R"
   },
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for btach in valid_loader:\n",
    "        userIdx = batch['userIdx']\n",
    "        click = batch['click']\n",
    "        scores_click = mf_model(userIdx, click)\n",
    "        neg_sample = torch.randint(0, mf_model.num_items, click.size(), device = click.device)\n",
    "        scores_noclick = mf_model(userIdx, neg_sample)\n",
    "\n",
    "        all_scores.append(scores_click)\n",
    "        all_labels.append(torch.ones_like(scores_click))\n",
    "\n",
    "        all_scores.append(scores_noclick)\n",
    "        all_labels.append(torch.zeros_like(scores_noclick))\n",
    "all_scores = torch.cat(all_scores).view(-1)\n",
    "all_labels = torch.cat(all_labels).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glf4DpM9UD2R"
   },
   "outputs": [],
   "source": [
    "auc_r = auroc(all_scores, all_labels.int(), task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ODdpYHQUD2S",
    "outputId": "7fce16aa-7886-4d7e-bbc5-db3df68b57b1"
   },
   "outputs": [],
   "source": [
    "auc_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDbhuNzTUD2S",
    "outputId": "f707bf35-b3b8-4a44-fd0e-a121065dd537"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_labels.cpu().numpy(), all_scores.cpu().numpy())\n",
    "roc_auc = auc(list(fpr), list(tpr))\n",
    "plt.figure()\n",
    "plt.plot(list(fpr), list(tpr), color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HcghC7hUD2S"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "predict_results = (all_scores > 0.5)\n",
    "predict_results = predict_results.numpy()\n",
    "all_labels = all_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwg9ZkXgUD2T",
    "outputId": "035ecccc-ba4f-4e71-fb2b-11891b721f90"
   },
   "outputs": [],
   "source": [
    "precision_score(predict_results, all_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uTKVPCjUD2T",
    "outputId": "f34194e0-fd4e-4ab0-eada-564f77e1eeac"
   },
   "outputs": [],
   "source": [
    "recall_score(predict_results, all_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eK7OWU5mUD2T",
    "outputId": "95a60979-bcef-4670-d1d0-0dd9705ea50a"
   },
   "outputs": [],
   "source": [
    "f1_score(predict_results, all_labels, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FDraMyMW0Gf"
   },
   "source": [
    "## Model: GNN(LightGCN) attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COsQHm3lUEPe"
   },
   "source": [
    "### Problem\n",
    "The recommendation problem addressed here involves recommending news articles to users. The system will be deployed within a mobile news application. Competitor analysis indicates that current systems, such as those used by popular news apps, often rely on collaborative filtering and suffer from the cold start problem. Our proposed system uses Graph Neural Networks (GNN) to leverage both user-article interaction data and content features for more accurate recommendations.\n",
    "\n",
    "- User Inputs: User interaction data (clicks, likes, shares) and user profile information.\n",
    "- Recommendations: News articles personalized to user preferences.\n",
    "- User Feedback: Clicks on recommended articles, which will be used to update the model.\n",
    "- Problem Definition: Ranking problem where the goal is to rank news articles according to user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUdKtkRMUEPi"
   },
   "source": [
    "Firstly, we import the packages that we will used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP25gOhFUEPj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
    "from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.models.deeprec.deeprec_utils import cal_metric\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfyCGle0UEPk"
   },
   "source": [
    "### Dataset\n",
    "<p> Dataset: MIND (Microsoft News Dataset), a large-scale dataset for news recommendation.</p>\n",
    "Characteristics:\n",
    "\n",
    "Contains news articles, user interaction data, and user profiles.\n",
    "Articles have associated metadata like title, abstract, and category.\n",
    "Exploratory Data Analysis:\n",
    "\n",
    "Distribution of interactions per user.\n",
    "Most popular categories of news articles.\n",
    "- Strengths: Rich user interaction data and comprehensive metadata for news articles.\n",
    "- Weaknesses: May have sparse interactions for some users (cold start problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfnhmJUGUEPl",
    "outputId": "e60195a1-caa2-429f-869e-22d1097b253d"
   },
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "raw_behaviour = pd.read_csv(\n",
    "    os.path.join(data_path, \"behaviors.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    names=[\"impressionId\",\"userId\",\"timestamp\",\"click_history\",\"impressions\"])\n",
    "\n",
    "print(f\"The dataset originally consist of {len(raw_behaviour)} number of interactions.\")\n",
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfNfuIlfUEPm"
   },
   "outputs": [],
   "source": [
    "# use-item click\n",
    "clicks = []\n",
    "for _, row in raw_behaviour.iterrows():\n",
    "    user_id = row[\"userId\"]\n",
    "    impressions = row[\"impressions\"].split()\n",
    "    for impression in impressions:\n",
    "        item_id, click = impression.split('-')\n",
    "        clicks.append((user_id, item_id, int(click)))\n",
    "df_clicks = pd.DataFrame(clicks, columns=['userID', 'itemID', 'rating'])\n",
    "#save data [click==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPgexHWhUEPn",
    "outputId": "49282bca-8f4f-4944-c8cf-7ea77c0822e6"
   },
   "outputs": [],
   "source": [
    "df_clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyQbTfv-UEPn"
   },
   "source": [
    "### Method\n",
    "\n",
    "#### Overall Approach:\n",
    "- Use a Graph Neural Network (GNN) for recommendation.\n",
    "<p> Construct a graph where nodes represent users and articles, and edges represent interactions.<p>\n",
    "- Method:\n",
    " We offer an example to help users to run a ID-based collaborative filtering baseline with LightGCN.\n",
    "LightGCN is a simple and neat Graph Convolution Network (GCN) model for recommender systems.\n",
    "I It uses a GCN to learn the embeddings of users/items, with the goal that low-order and high-order user-item interactions are explicitly exploited into the embedding function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9H0foK9UEPn"
   },
   "source": [
    "![jupyter](https://camo.githubusercontent.com/d01f9da6d6cf4e07e35b0d77ccbf0195851ef1b1a035c30efc810e37b0da624a/68747470733a2f2f7265636f64617461736574732e7a32302e7765622e636f72652e77696e646f77732e6e65742f6b6464323032302f696d616765732532464c6967687447434e2d67726170686578616d706c652e4a5047)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oBwC_yCUEPo"
   },
   "source": [
    "Model structure as belows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY46JEU5UEPo"
   },
   "source": [
    "![jupyter](https://camo.githubusercontent.com/f390dcd24e48a86a2c6eeac9344eac60f9b45fc817bc24cf850e7d28d8a955bd/68747470733a2f2f7265636f64617461736574732e7a32302e7765622e636f72652e77696e646f77732e6e65742f696d616765732f6c6967687447434e2d6d6f64656c2e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhdv4p6xUEPo"
   },
   "source": [
    "LightGCN only takes positive user-item interactions for model training. Pairs with rating < 1 will be ignored by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdaQoH2KUEPo"
   },
   "outputs": [],
   "source": [
    "data = ImplicitCF(\n",
    "    train=df_clicks, test=df_clicks, seed=0,\n",
    "    col_user='userID',\n",
    "    col_item='itemID',\n",
    "    col_rating='rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7qzXlcaUEPp"
   },
   "outputs": [],
   "source": [
    "yaml_file = './lightgcn.yaml'\n",
    "lightgcn_dir = './lightgcn_model'\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "create_dir(lightgcn_dir)\n",
    "hparams = prepare_hparams(yaml_file,\n",
    "                          learning_rate=0.005,\n",
    "                          eval_epoch=20,\n",
    "                          top_k=20,\n",
    "                          save_model=False,\n",
    "                          epochs=300,\n",
    "                          save_epoch=20\n",
    "                         )\n",
    "hparams.MODEL_DIR = os.path.join(lightgcn_dir, 'saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lpt0tfXUEPp",
    "outputId": "39dd428e-be95-4b74-c039-b25346ead329"
   },
   "outputs": [],
   "source": [
    "model = LightGCN(hparams, data, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6t7CTnFUEPp",
    "outputId": "f70a5f88-4363-45f0-edc4-caf4fd192097"
   },
   "outputs": [],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit()\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z3-xOvgUEPp"
   },
   "source": [
    "### Reflection\n",
    "\n",
    "\n",
    "GNN seems not producing a reasonable result by this trying. It may be the issues with the establishing the nodes and the features regarding the users. As the dataset do not contain much demographics other than click information, we may need to further gather the data and modify the GCN model.\n",
    "\n",
    "##### Challenges:\n",
    "\n",
    "Computationally intensive training.\n",
    "Addressing the cold start problem for new users and articles.\n",
    "\n",
    "##### Future Work:\n",
    "\n",
    "Integrate context-aware recommendation to consider user’s current context.\n",
    "Explore sequential recommendation to account for temporal dynamics in user interactions.\n",
    "Incorporate social network data for enhanced recommendations.\n",
    "Commercial Viability:\n",
    "\n",
    "The proposed system shows promise but requires further optimization for real-time recommendations and handling new user/article scenarios.\n",
    "\n",
    "##### Reference:\n",
    "\n",
    "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation https://arxiv.org/abs/2002.02126\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1iSvu-iYg9A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTWBF9wTyqGR"
   },
   "source": [
    "## Model: Deep Learning Moedel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tby2wvu6y5Hg"
   },
   "source": [
    "#### Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tujv9-CqyVTn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_uV4tg9y60n"
   },
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_JDHAUgy2m9"
   },
   "outputs": [],
   "source": [
    "def load_data(behaviors_path, news_path):\n",
    "    behaviors = pd.read_csv(behaviors_path, sep='\\t', names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "    news = pd.read_csv(news_path, sep='\\t', names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "    return behaviors, news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6Zot32nzAWZ"
   },
   "source": [
    "#### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-WwiUvwzDsc"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(entity_embedding_path, relation_embedding_path):\n",
    "    entity_embeddings = pd.read_csv(entity_embedding_path, sep=' ', header=None, index_col=0)\n",
    "    relation_embeddings = pd.read_csv(relation_embedding_path, sep=' ', header=None, index_col=0)\n",
    "    return entity_embeddings, relation_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_Zi70NxzG4H"
   },
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wISOKKMgzOiF"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(behaviors, news):\n",
    "    # Split user click history and impressions\n",
    "    behaviors['history'] = behaviors['history'].fillna('').apply(lambda x: x.split(' '))\n",
    "    behaviors['impressions'] = behaviors['impressions'].apply(lambda x: [i.split('-') for i in x.split(' ')])\n",
    "\n",
    "    # Fill missing values in news data\n",
    "    news['title'] = news['title'].fillna('')\n",
    "    news['abstract'] = news['abstract'].fillna('')\n",
    "    news['category'] = news['category'].fillna('unknown')\n",
    "    news['subcategory'] = news['subcategory'].fillna('unknown')\n",
    "    return behaviors, news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjKvF3KQzGcE"
   },
   "source": [
    "#### Text representation using TF-IDF\n",
    "\n",
    "We adopted the TF-IDF method, setting the maximum number of features to 5000, and combined news titles and abstracts for vectorization. This method effectively captures the importance of words, particularly suitable for text-intensive content like news. Due to computational power limitations, we didn't use the BERT model, as the subsequent cosine similarity calculations were too computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYoAqhM8zR5f"
   },
   "outputs": [],
   "source": [
    "def text_representation(news):\n",
    "    # Combine title and abstract for TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    news['title_abstract'] = news['title'] + ' ' + news['abstract']\n",
    "    tfidf_matrix = vectorizer.fit_transform(news['title_abstract'])\n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myUtDqKCzTNp"
   },
   "source": [
    "#### Entity representation using embeddings\n",
    "\n",
    "we extracted entities mentioned in each news article and used pre-trained entity embeddings to represent these entities. By averaging all entity embeddings for each news article, we obtained the entity representation for that news. This method allows our system to understand the semantic content of the news, not just the surface text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1M_3wlPzVWx"
   },
   "outputs": [],
   "source": [
    "def entity_representation(news, entity_embeddings):\n",
    "    embedding_dim = entity_embeddings.shape[1]\n",
    "    news_entity_embeddings = np.zeros((len(news), embedding_dim))\n",
    "\n",
    "    for i, entities in enumerate(news['title_entities']):\n",
    "        if pd.isna(entities) or entities == \"[]\":\n",
    "            continue\n",
    "        entity_ids = [entity['WikidataId'] for entity in eval(entities) if 'WikidataId' in entity]\n",
    "        if len(entity_ids) > 0:\n",
    "            embeddings = np.mean([entity_embeddings.loc[e].values for e in entity_ids if e in entity_embeddings.index], axis=0)\n",
    "            news_entity_embeddings[i] = embeddings\n",
    "\n",
    "    return news_entity_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1XHnHeizYzO"
   },
   "source": [
    "#### Combine TF-IDF and entity embeddings\n",
    "\n",
    "we concatenated the TF-IDF vectors and entity embedding vectors. The advantage of this method is that it considers both the statistical characteristics of the text and semantic information, providing rich feature representation for each news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Apr2bpM6zWv9"
   },
   "outputs": [],
   "source": [
    "def combine_features(tfidf_matrix, news_entity_embeddings, news):\n",
    "    category_encoder = OneHotEncoder()\n",
    "    categories = category_encoder.fit_transform(news[['category', 'subcategory']]).toarray()\n",
    "    return np.hstack((tfidf_matrix.toarray(), news_entity_embeddings, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIT_dWcgzcyu"
   },
   "source": [
    "#### User profile modeling\n",
    "\n",
    "we adopted a simple but effective method. We traversed each user's click history, extracted feature vectors of all clicked news, and then took the average as the user's interest representation. This method effectively captures the overall interest distribution of users and is computationally efficient, suitable for large-scale online recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFqfTN8QzkNz"
   },
   "outputs": [],
   "source": [
    "def user_profile_modeling(behaviors, news, combined_features, aggregate_size=100):\n",
    "    news_index = {news_id: idx for idx, news_id in enumerate(news['news_id'])}\n",
    "    user_profiles = {}\n",
    "\n",
    "    def aggregate_profiles(profiles):\n",
    "        return np.mean(profiles, axis=0) if len(profiles) > 0 else np.zeros(combined_features.shape[1])\n",
    "\n",
    "    for user_id, hist in zip(behaviors['user_id'], behaviors['history']):\n",
    "        if user_id not in user_profiles:\n",
    "            user_profiles[user_id] = []\n",
    "        weights = np.arange(1, len(hist) + 1) / len(hist)  # increasing weights for more recent items\n",
    "        for news_id, weight in zip(hist, weights):\n",
    "            if news_id in news_index:\n",
    "                news_idx = news_index[news_id]\n",
    "                user_profiles[user_id].append(weight * combined_features[news_idx])\n",
    "            if len(user_profiles[user_id]) >= aggregate_size:\n",
    "                user_profiles[user_id] = [aggregate_profiles(user_profiles[user_id])]\n",
    "\n",
    "    for user_id in user_profiles:\n",
    "        if len(user_profiles[user_id]) > 0:\n",
    "            user_profiles[user_id] = aggregate_profiles(user_profiles[user_id])\n",
    "        else:\n",
    "            user_profiles[user_id] = np.zeros(combined_features.shape[1])\n",
    "\n",
    "    return user_profiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpoiI7daziaS"
   },
   "source": [
    "#### Recommendation generation\n",
    "\n",
    "We use cosine similarity to match user profiles and candidate news articles. Specifically, we calculate the cosine similarity between the user profile vector and all candidate news vectors, then select the top N articles with the highest similarity as the recommendation results. This method is simple, intuitive, and fast to compute, very suitable for real-time recommendation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqbA2phRzniC"
   },
   "outputs": [],
   "source": [
    "def generate_recommendations(user_profiles, news, combined_features):\n",
    "    news_index = {news_id: idx for idx, news_id in enumerate(news['news_id'])}\n",
    "    recommendations = {}\n",
    "\n",
    "    for user_id in tqdm(user_profiles):\n",
    "        user_profile = user_profiles[user_id]\n",
    "        cosine_similarities = cosine_similarity(user_profile.reshape(1, -1), combined_features)\n",
    "        similar_indices = cosine_similarities.argsort().flatten()[-10:]\n",
    "        recommendations[user_id] = [news['news_id'][i] for i in similar_indices]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYyeJAkezqBV"
   },
   "source": [
    "#### Define PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXmzddXvzyeW"
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, news, labels):\n",
    "        self.news = news\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.news)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        news = self.news[idx]\n",
    "        label = self.labels[idx]\n",
    "        return news, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPanwMf9z1BC"
   },
   "source": [
    "#### Define the neural network model\n",
    "\n",
    "We introduced a deep learning model using PyTorch. This feedforward neural network includes an input layer, two hidden layers with Batch Normalization and ReLU activation, and Dropout layers to prevent overfitting. This model aims to learn complex relationships between user interests and news content, enhancing recommendation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajbDzDpBzh3B"
   },
   "outputs": [],
   "source": [
    "class NewsRecommendationModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_prob=0.5):\n",
    "        super(NewsRecommendationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAIROXmgz39O"
   },
   "source": [
    "#### Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw2EjCNpz7J4"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloader, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.float(), labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH1TWSZOz9SB"
   },
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOamwVWRz8g9"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(recommendations, behaviors):\n",
    "    all_true_labels = []\n",
    "    all_pred_scores = []\n",
    "\n",
    "    for user_id, impressions in zip(behaviors['user_id'], behaviors['impressions']):\n",
    "        if user_id in recommendations:\n",
    "            user_recommendations = recommendations[user_id]\n",
    "            for impression in impressions:\n",
    "                news_id, label = impression\n",
    "                label = int(label)\n",
    "                score = 1 if news_id in user_recommendations else 0\n",
    "                all_true_labels.append(label)\n",
    "                all_pred_scores.append(score)\n",
    "\n",
    "    auc = roc_auc_score(all_true_labels, all_pred_scores)\n",
    "    map_score = average_precision_score(all_true_labels, all_pred_scores)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"MAP: {map_score}\")\n",
    "\n",
    "    return auc, map_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUfwnB5e0FCE"
   },
   "source": [
    "#### Output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6xUDEU80A2s"
   },
   "outputs": [],
   "source": [
    "def output_predictions(recommendations, behaviors, output_path='predictions.txt'):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for user_id, impressions in zip(behaviors['user_id'], behaviors['impressions']):\n",
    "            if user_id in recommendations:\n",
    "                user_recommendations = recommendations[user_id]\n",
    "                pred_rank = (np.argsort(np.argsort(user_recommendations)[::-1]) + 1).tolist()\n",
    "                pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "                f.write(' '.join([str(user_id), pred_rank]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ono2feXF0I04"
   },
   "source": [
    "#### Load data and embeddings, Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFwmV_q80QQw"
   },
   "outputs": [],
   "source": [
    "behaviors, news = load_data('/content/drive/MyDrive/MIND/behaviors.tsv', '/content/drive/MyDrive/MIND/news.tsv')\n",
    "entity_embeddings, relation_embeddings = load_embeddings('/content/drive/MyDrive/MIND/entity_embedding.vec', '/content/drive/MyDrive/MIND/relation_embedding.vec')\n",
    "behaviors, news = preprocess_data(behaviors, news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g54wfQ090XCY"
   },
   "source": [
    "#### Create TF-IDF and entity embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHgbuEv70WF-"
   },
   "outputs": [],
   "source": [
    "tfidf_matrix, vectorizer = text_representation(news)\n",
    "news_entity_embeddings = entity_representation(news, entity_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndl1yUmZ0dl1"
   },
   "source": [
    "#### Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eqdyx_a0kCv"
   },
   "outputs": [],
   "source": [
    "combined_features = combine_features(tfidf_matrix, news_entity_embeddings, news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VHxwYvj0is5"
   },
   "source": [
    "#### Create user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXfwLfOi0m3V"
   },
   "outputs": [],
   "source": [
    "user_profiles = user_profile_modeling(behaviors, news, combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAdQM-y10on5"
   },
   "source": [
    "#### Generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0Udx2Ld0uDW"
   },
   "outputs": [],
   "source": [
    "recommendations = generate_recommendations(user_profiles, news, combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52yuFPIH0yBK"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzkzndnO01iK"
   },
   "outputs": [],
   "source": [
    "input_dim = combined_features.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "model = NewsRecommendationModel(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chcc3B4I000Z"
   },
   "source": [
    "#### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqdBELvr03ZB"
   },
   "outputs": [],
   "source": [
    "all_news = torch.tensor(combined_features)\n",
    "labels = torch.tensor([0] * len(all_news))  # Dummy labels for all news items\n",
    "dataset = NewsDataset(all_news, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldTba2uJ059B"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANETBgwP09jq",
    "outputId": "16006f6e-e77a-45c8-c1e6-b5913de5fd49"
   },
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, dataloader, num_epochs=5)\n",
    "evaluate_model(recommendations, behaviors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpLlrED13Zr"
   },
   "source": [
    "## Disscusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jYQ_XXq1_Ts"
   },
   "source": [
    "1. **TF-IDF and Entity Embedding**:\n",
    "   - **Manual Feature Design and Extraction**: Requires significant manual effort for feature design and extraction.\n",
    "   - **Complex Relationships**: May struggle to capture complex relationships due to the manual nature of feature engineering.\n",
    "\n",
    "2. **Matrix Factorization (MF)**:\n",
    "   - **Handling Sparse and Incomplete Data**: Effective in managing sparse and incomplete datasets.\n",
    "   - **Dimensionality Reduction**: Reduces the dimensionality and complexity of the data.\n",
    "   - **Modeling Latent Factors**: Efficient in modeling latent factors in user-item interactions.\n",
    "   - **Overfitting and Underfitting**: Susceptible to overfitting and underfitting, which can impact accuracy.\n",
    "\n",
    "3. **Neural Collaborative Filtering (NCF)**:\n",
    "   - **Understanding High-Dimensional Sparse Data**: Capable of comprehending high-dimensional sparse data.\n",
    "   - **Embeddings for Interactions**: Uses embeddings to capture interactions and extract features.\n",
    "   - **Deep Learning Approach**: Employs deep learning techniques to enhance collaborative filtering.\n",
    "\n",
    "4. **LibFM (Factorization Machines)**:\n",
    "   - **High-Dimensional Sparse Data**: Performs well with high-dimensional sparse datasets.\n",
    "   - **Feature Interactions**: Captures feature interactions, offering more expressiveness than linear models.\n",
    "\n",
    "5. **Graph Neural Networks (GNNs)**:\n",
    "   - **Capturing Complex High-Order Relationships**: Excellent at modeling complex high-order relationships between users and news articles.\n",
    "   - **Deeper Interactions**: Utilizes message-passing mechanisms to model deeper interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_OQJXji2Yl4"
   },
   "source": [
    "### Summary\n",
    "\n",
    "- **TF-IDF and Entity Embedding**: Labor-intensive with limitations in capturing complex relationships.\n",
    "- **Matrix Factorization (MF)**: Effective for sparse data and latent factor modeling but prone to overfitting and underfitting.\n",
    "- **Neural Collaborative Filtering (NCF)**: Handles high-dimensional sparse data using deep learning and embeddings.\n",
    "- **LibFM (Factorization Machines)**: Captures feature interactions effectively in high-dimensional sparse data.\n",
    "- **Graph Neural Networks (GNNs)**: Excels in capturing complex relationships and deeper interactions through advanced mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "1.\tEnhanced Feature Engineering: Explore automated methods for feature design and extraction to reduce manual effort and improve the ability to capture complex relationships.\n",
    "2.\tHybrid Models: Develop hybrid approaches that combine matrix factorization with other techniques to mitigate overfitting and underfitting while improving predictive accuracy.\n",
    "3.\tContextual Information Integration: Incorporate additional contextual information such as user demographics, temporal dynamics, and item attributes to enhance model performance.\n",
    "4.\tScalability Improvements: Investigate scalable algorithms and distributed computing techniques to handle large-scale datasets more efficiently.\n",
    "5.\tAdvanced Neural Network Techniques: Apply advanced neural network techniques, such as dynamic and hierarchical graph neural networks, to better capture complex and evolving relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1zIyWRV2atZ"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully implemented and evaluated a news recommendation system based on the MIND dataset. By utilizing a variety of methodologies, we were able to analyze their effectiveness in recommending news articles to users based on their historical behaviors.\n",
    "\n",
    "Through this exploration, we identified the strengths and weaknesses of each approach, providing valuable insights into their application in news recommendation systems. The diversity of methods, from traditional techniques like TF-IDF and entity embedding to more advanced approaches like GNNs, highlighted the complexity and nuance required in creating effective recommendation systems.\n",
    "\n",
    "Our findings suggest that while each method has its advantages, there is significant potential for improvement, particularly in areas such as feature engineering, hybrid model development, and scalability. These improvements could enhance the system's ability to provide personalized and accurate recommendations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lEs7NR9ryNOr",
    "Ds7sZALpyNOv",
    "aZWLqCjayNOw",
    "EYvYo-BoyNOx",
    "WlUWvcCtyNOy",
    "IQVvfemNSZQT",
    "dS7b3FHOUYHD",
    "8FDraMyMW0Gf",
    "RTWBF9wTyqGR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
