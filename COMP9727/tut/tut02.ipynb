{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw64x46dX49"
   },
   "source": [
    "# **COMP9727 Recommender Systems**\n",
    "## Tutorial Week 2: Topic Classification\n",
    "\n",
    "@Author: **Mingqin Yu**\n",
    "\n",
    "@Reviewer: **Wayne Wobcke**\n",
    "\n",
    "### Objective\n",
    "\n",
    "The aim of the tutorial is to become more familiar with text processing pipelines for use in topic classification and content-based recommendation, focusing on Naive Bayes models for text classification. This will be important for the assignment, where this will form the basis of a recommender system.\n",
    "\n",
    "### Before the tutorial\n",
    "\n",
    "1. Look up regex, in particular the regular expression syntax. This will be used to identify \"words\" in the documents prior to classification.\n",
    "\n",
    "2. Review the lecture material on Naive Bayes text classification and metrics for evaluation of classification models.\n",
    "\n",
    "3. Read, understand and run all the code in the NLP pipeline below, and come prepared to discuss the answers to some of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrXoTPuL87-p"
   },
   "source": [
    "### Overview of NLP Pipeline\n",
    "\n",
    "1. **Loading the Dataset**\n",
    "    - Load a subset of the '20 newsgroups' dataset\n",
    "2. **Initial Data Cleansing**\n",
    "    - Remove duplicate and missing values\n",
    "2. **Data Preprocessing**\n",
    "    - Convert the text to lower case\n",
    "    - Remove punctuation and other characters\n",
    "    - Tokenize the text\n",
    "    - Remove stopwords\n",
    "    - Stem the words\n",
    "3. **Feature Extraction**\n",
    "    - Convert the preprocessed text data into tf-idf values\n",
    "4. **Construct the Training and Test Sets**\n",
    "    - Split the data into 80% training and 20% test sets\n",
    "5. **Model Training**:\n",
    "    - Train a Naive Bayes model on the training data\n",
    "6. **Model Evaluation**\n",
    "    - Predict the categories of instances in the test set\n",
    "    - Evaluate the model's performance using accuracy and the classification report\n",
    "\n",
    "**Specific Learning Objectives**\n",
    "\n",
    "1. **Understand and Implement Data Preprocessing**\n",
    "    - Understand the importance of data preprocessing in Natural Language Processing tasks\n",
    "    - Implement text preprocessing steps such as lowercasing, removing punctuation, tokenization, removing stopwords and stemming\n",
    "2. **Understand and Implement Text Vectorization**\n",
    "    - Understand the use of tf-idf (Term Frequency-Inverse Document Frequency) vectors\n",
    "    - Implement text vectorization using the `TfidfVectorizer` from `sklearn.feature_extraction.text`\n",
    "3. **Understand and Implement Model Training and Evaluation**\n",
    "    - Understand the basics of Naive Bayes classifiers\n",
    "    - Implement model training using the `BernoulliNB` class from `sklearn.naive_bayes`\n",
    "    - Evaluation the model using `accuracy_score` and `classification_report` from `sklearn.metrics`\n",
    "4. **Understand and Implement Data Splitting**:\n",
    "    - Understand the importance of splitting the data into training and test sets\n",
    "    - Implement data splitting using the `train_test_split` function from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M4LJ_S0d5g1"
   },
   "source": [
    "## NLP Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS9MX9WUS-fp"
   },
   "source": [
    "### 1. Introduction\n",
    "\n",
    "Text classification is a crucial task in Natural Language Processing (NLP) with various applications such as sentiment analysis, spam detection and topic classification. This tutorial will guide you through the process of building a text classification model using the Bernoulli Naive Bayes algorithm. We will start with text preprocessing and feature extraction, followed by model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn5CwHANJ2cO"
   },
   "source": [
    "### 2. Data Preprocessing\n",
    "\n",
    "Raw text data is unstructured and noisy. It contains a lot of unwanted characters, punctuation and stopwords. Data preprocessing is essential to clean and structure the text data before feeding it into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgHjJeKQIL6n"
   },
   "source": [
    "__Step 1. Load the Dataset__\n",
    "\n",
    "* The first step is to load the data. We are using the '20 newsgroups' dataset, which is a collection of approximately 20,000 newsgroup documents, spanning 20 different newsgroups. We fetch a small subset of the dataset with only three categories: 'rec.autos', 'sci.med', 'comp.graphics'.\n",
    "* The `fetch_20newsgroups` function from `sklearn.datasets` is used to fetch the dataset. The data is put into a Python dataframe.\n",
    "* Next we drop any duplicate rows and rows with missing values from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP_hPmjeTzMw"
   },
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "# Check the sklearn documentation for details\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch a subset of the 20 newsgroups dataset\n",
    "categories = ['rec.autos', 'sci.med', 'comp.graphics']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'Content': newsgroups.data, 'Category': newsgroups.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVdkXrfyIeKc",
    "outputId": "c2dd71d0-d6e4-49be-a911-12460693961c"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 180)\n",
    "pd.set_option('display.max_colwidth', 140)\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3kzfaBe6OmZ"
   },
   "source": [
    "__Step 2. Initial Data Cleansing__\n",
    "\n",
    "Duplicate and missing values need to be removed before training the model, though there should be none in this dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkHBgGVl6XK7"
   },
   "outputs": [],
   "source": [
    "# Drop duplicates and missing values\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QcaZbE-J3ID",
    "outputId": "a1e61f01-d853-4120-c803-b2eeef18d7ce"
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAGgGeSI6fux"
   },
   "source": [
    "__Step 3. Text Preprocessing__\n",
    "\n",
    "The raw text data is unstructured and contains a lot of noise such as punctuation, stopwords and different cases. Preprocessing the text data helps in removing this noise and converting the text into a structured form. Here the text data is preprocessed using the Natural Language Toolkit (NLTK). A preprocessing function `preprocess_text` is defined which carries out all these preprocessing steps. This function is then applied to each document in the dataframe.\n",
    "\n",
    "1. **Regular Expressions (`re` library)**: Regular expressions are used for specifying text patterns. We use the `re.sub` function from the `re` library to remove punctuation and special characters from the text. This is important because those characters might introduce noise into the data. On the other hand, some of these characters might be useful for classification.\n",
    "\n",
    "2. **Lowercasing**: This process involves converting all the characters in the text to lowercase using the `lower` function. This step can help in treating words like 'The' and 'the' as the same word, which is usually the desired behaviour in text analysis. However, some information pertaining to proper names might be lost, such as when 'Apple' is reduced to 'apple'.\n",
    "\n",
    "3. **Tokenization**: This is the process of splitting text into words (tokens) using the `word_tokenize` function from the `nltk.tokenize` module. This is a fundamental step because text data needs to be tokenized into words for analysis.\n",
    "\n",
    "4. **Stopword Removal**: Stopwords are common words in a language (e.g. 'the', 'a', 'and', etc.) that are usually removed from the text during preprocessing. Here we use the `stopwords.words` function from the `nltk.corpus` module to get a list of stopwords in English. This step is crucial because it helps in focusing on the important words by removing the most common words that do not carry much information, similar to the use of tf-idf features.\n",
    "\n",
    "5. **Stemming/Lemmatization**: This process involves converting a word to a shortened form, here using the `PorterStemmer` class from the `nltk.stem` module. This can help reduce the dimensionality of the text data and may result in more meaningful words, however may also combine different words with the same stem, losing the distinction between them, such as with 'machine' and 'machinery'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LIM1gSaQpg_",
    "outputId": "6b86d3e0-fc91-4110-f7b3-12258851d768"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)     # Check what this removes --- might be too much!\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "df['Content'] = df['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzFA4CcYKaJ3"
   },
   "outputs": [],
   "source": [
    "print(df['Content'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training and Evaluation\n",
    "\n",
    "Now the model can be constructed. The first step is feature extraction, in this case converting each document to a vector of tf-idf values, followed by training a Bernoulli Naive Bayes model and evalation of the model using various metrics and plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC0ZxtW8Q4Ly"
   },
   "source": [
    "__Step 1. Feature Extaction__\n",
    "\n",
    "Here tf-idf features are used to convert each document into a numerical vector using the `TfidfVectorizer` function from `sklearn.feature_extraction.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPW6Isn2RHEo"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text data into TF-IDF weights\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsSqANr_fuW_",
    "outputId": "77bfc089-b32b-4e53-883a-9244b4ebfe10"
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoF2Ta9gUtb_"
   },
   "source": [
    "__Step 2. Construct the Training and Test Sets__\n",
    "\n",
    "The data is then split randomly into training and testing sets using the `train_test_split` function from `sklearn.model_selection`, with 80% of the data used for training and 20% used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYCdr9FnVCeH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['Category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5MHet_Pf38O",
    "outputId": "f7875d80-d79d-49f9-8eb3-b6378b712655"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q5gA0ueXuF7"
   },
   "source": [
    "__Step 3. Model Training__\n",
    "\n",
    "A Bernoulli Naive Bayes model is trained using the `BernoulliNB` class from `sklearn.naive_bayes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "G6jbNgHLMdiL",
    "outputId": "c6ce23fc-082b-49c7-e7a0-11e37f1da6c0"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Train the Bernoulli Naive Bayes model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twAGTz-hMfRm",
    "outputId": "dd904ca6-529b-4be6-a18c-5ba898e147ad"
   },
   "outputs": [],
   "source": [
    "print(bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCjhaQYCXrOR"
   },
   "source": [
    "__Step 4. Model Evaluation__\n",
    "\n",
    "The trained model is then used to predict the categories of the test set. The model's performance is evaluated using accuracy and the classification report, which includes precision, recall and F1. These metrics are calculated using the `accuracy_score` and `classification_report` functions from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQCoe_NRMk9e",
    "outputId": "d8eb8a74-dba6-4018-927b-d787628ea452"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict the categories of the test set\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VRZPzGvrPKQ"
   },
   "source": [
    "__Step 5. Interpretation of Results__\n",
    "\n",
    "To interpret the results, it is often useful to compare performance on the training and test sets, and some information is easier to understand in graph form. One Python library useful for generating plots is `matplotlib`.\n",
    "\n",
    "The following is one example usage. \n",
    "* `import numpy as np`: This imports the NumPy library, which is used for numerical operations in Python, here for handling arrays and performing array operations.\n",
    "* `import matplotlib.pyplot as plt`: This imports the plotting library that we use to create the bar plot.\n",
    "* `np.unique(y_test, return_counts=True)`: Here we use NumPy's unique function to find the unique elements of y_test and y_pred (the true labels and the predicted labels), and count their occurrences.\n",
    "* `plt.figure(figsize=(12,6))`: This function creates a new figure for the plot with the specified figure size to make sure all elements fit nicely and are clearly visible.\n",
    "* `plt.bar`: This function is used to create bar plots for both the true and the predicted label distributions. We create two sets of bars, one for the true labels and one for the predicted labels, with different colours for each.\n",
    "* `plt.xticks`: This function is used to set the labels for the x-axis using the category names, and we rotate the labels by 45 degrees to avoid overlapping.\n",
    "* `plt.legend()`: This adds a legend to the plot to differentiate between true and predicted bars.\n",
    "* `plt.tight_layout()`: This function ensures that all elements of the plot are displayed properly without overlapping.\n",
    "* `plt.savefig('topic_distribution.png')`: This function saves the plot as a PNG file.\n",
    "* `plt.show()`: Finally, this function displays the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "L_iLNMl3qtUY",
    "outputId": "74595062-589d-429e-b454-bab587785e29"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of the actual topics in the test set\n",
    "unique_true, counts_true = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# Plot the distribution of the predicted topics\n",
    "unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# Create bar width\n",
    "barWidth = 0.2\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(counts_true))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Create subplot for 'true' distribution\n",
    "plt.bar(r1, counts_true, color='b', width=barWidth, edgecolor='white', label='True')\n",
    "\n",
    "# Create subplot for 'predicted' distribution\n",
    "plt.bar(r2, counts_pred, color='c', width=barWidth, edgecolor='white', label='Predicted')\n",
    "\n",
    "plt.xlabel('Topic', fontweight='bold')\n",
    "plt.ylabel('Frequency', fontweight='bold')\n",
    "plt.title('Frequency Distribution of True and Predicted Topics', fontweight='bold')\n",
    "\n",
    "plt.xticks([r + barWidth/2 for r in range(len(counts_true))], categories, rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feoiaVj1eE_t"
   },
   "source": [
    "## Discussion Questions\n",
    "\n",
    "Experiment with the above code before class and think about the answers to the following questions. It does not matter if you do not do all the questions; the main thing is to do some of them and come to class ready for the discussion.\n",
    "\n",
    "1. **Data Preprocessing**\n",
    "    - Is the order of the steps in data preprocessing important? What happens if you change the order?\n",
    "    - Compare the model's performance on all metrics with and without stopword removal. Are the results much different?\n",
    "    - See if you can find the stopword list used by `nltk`. What do you think of it?\n",
    "    - Find out how to use the English stopword list with `sklearn` and see if the results are better or worse.\n",
    "    - Discuss the advantages and disadvantages of stemming. Would lemmatization be a better option for this problem?\n",
    "2. **Feature Extraction**\n",
    "    - What is the definition of *word* used by `TfidfVectorizer` in `sklearn`? Is there a better definition you can think of? Define the regex.\n",
    "    - Look carefully at the parameters of `TfidfVectorizer`. How do they affect the performance of the models?\n",
    "    - Try replacing `TfidfVectorizer` by `CountVectorizer`, which simply records word counts (term frequencies). Are the results better or worse?\n",
    "3. **Model Training**\n",
    "    - Discuss the parameters of `BernoulliNB`. How do they affect the model's performance?\n",
    "    - Here we chose the Bernoulli Naive Bayes classifier for this problem. Does Multinomial Naive Bayes work better? Which is supposed to work better?\n",
    "    - Are the classes balanced or imbalanced? Is this realistic? Plot the distribution of class labels in the full dataset to show the distribution.\n",
    "    - How well do the Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers work when the classes are imbalanced? Try some experiments.\n",
    "    - This seems to be a particularly \"easy\" classification task. Try the model with more of the 20 categories? Can the good results be replicated?\n",
    "    - In this dataset, each document is given exactly one \"ground truth\" label. How could we handle the situation where some documents have no label?\n",
    "4. **Model Evaluation**\n",
    "    - Discuss the performance of the Bernoulli Naive Bayes model. Is the model performing well? What can be done to improve its performance?\n",
    "    - Here we used only one training-test split. Should we have used cross-validation? Check the `sklearn` documentation to find out how to do this.\n",
    "    - Discuss the evaluation metrics chosen. Are there other metrics that would provide more insights into the performance of the models?\n",
    "    - What do these results tell us about how to build a recommender system based on this model?\n",
    "5. **General Questions**\n",
    "    - Why do we need to classify into topics and subtopics? How well would the recommender system work if we just compared news articles directly?\n",
    "    - What sort of subtopics would be useful with the current classification scheme?\n",
    "    - How might the recommendations differ if we used a collaborative filtering approach instead of a content-based approach? What are the pros and cons?\n",
    "    - Can you think of any other potential applications of text classification and document similarity in the context of news articles?\n",
    "\n",
    "## Advanced\n",
    "\n",
    "1. Research one model not covered in class, such as Logistic Regression, Random Forests, SVM, etc. Which of these methods would be suitable for this topic classification task? Explain why.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
